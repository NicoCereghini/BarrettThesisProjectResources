{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3749,"status":"ok","timestamp":1679086435259,"user":{"displayName":"Nico Cereghini","userId":"07571101354927414086"},"user_tz":420},"id":"uhOf7XQENS7s","outputId":"b767bbce-a1e6-4b3d-c686-3d86804ee345"},"outputs":[{"output_type":"stream","name":"stdout","text":["GPU is available\n"]}],"source":["import torch\n","\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(\"GPU is available\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"GPU is not available\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10378,"status":"ok","timestamp":1679086445634,"user":{"displayName":"Nico Cereghini","userId":"07571101354927414086"},"user_tz":420},"id":"U2fcGy5kuL-U","outputId":"57938c1f-1bc4-4cec-d53b-e06391239e9f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"BeoCeogKyVbE"},"source":["Before running this notebook:\n","\n","\n","*   Import Utils folder from Github\n","*   Set Runtime to use GPU\n","*   Run code above w/out errors\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pk76LR7a7G0q"},"source":["Dataset Parsing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jDZay8Yi7ItO"},"outputs":[],"source":["import gzip\n","\n","def read_dataset(file_path):\n","    # Load the data from the file\n","    with gzip.open(file_path, 'rb') as f:\n","        data = f.read()\n","\n","    # Split the data into lines\n","    lines = data.strip().split(b'\\n')\n","\n","    # Split the lines into input data and target data\n","    input_data = []\n","    target_data = []\n","    for line in lines:\n","        # Remove leading and trailing whitespace and split on the tab character\n","        target_str, input_str = line.strip().split(b'\\t', 1)\n","        input_data.append(input_str.decode('utf-8'))\n","        target_data.append(target_str.decode('utf-8'))\n","    #The below line is very important, please do not forget this or colab will not work\n","    #return input_data[:len(input_data) // 4], target_data[:len(target_data) // 4]\n","    return input_data[:1000], target_data[:1000]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7wzQeLatDP0B"},"outputs":[],"source":["############################################################################################\n","\n","#VOCAB \n","def build_vocab(input_data, target_data):\n","    # Create a set to store the words\n","    vocab = set()\n","    vocab.add('<PAD>')\n","    # Loop over the input data and add the words to the vocab set\n","    for input_str in input_data:\n","        for word in input_str.split():\n","            vocab.add(word)\n","\n","    # Loop over the target data and add the words to the vocab set\n","    for target_str in target_data:\n","        for word in target_str.split():\n","            vocab.add(word)\n","\n","    return vocab\n","############################################################################################\n","# Word to Index Mappings\n","def create_word_to_index_mapping(vocab):\n","    # Sort the vocabulary\n","    vocab = sorted(vocab)\n","\n","    # Create a dictionary mapping words to integer indices\n","    word_to_index = {word: index for index, word in enumerate(vocab)}\n","\n","    return word_to_index\n","\n","\n","# Index to Word Mappings\n","def create_index_to_word_mapping(vocab):\n","    index_to_word = {}\n","    for i, word in enumerate(vocab):\n","        index_to_word[i] = word\n","\n","    return index_to_word\n","############################################################################################\n","\n","#Text to Sequence \n","def text_to_sequence(text, word_to_index):\n","    # Tokenize the text into words\n","    words = text.split()\n","\n","    # Map the words to integer indices\n","    indices = [word_to_index[word] for word in words]\n","\n","    return indices\n","\n","############################################################################################\n","\n","#Sequence to Text\n","def sequence_to_text(sequence, index_to_word):\n","    # Map the integer indices to words\n","    words = [index_to_word[index] for index in sequence]\n","\n","    # Join the words into a single string\n","    text = ' '.join(words)\n","\n","    return text\n","\n","############################################################################################\n","\n","#Sequence to sequences of word indices\n","def sequence_to_sequence_of_word_indices(sequences, word_to_index):\n","    # Convert the sequences to sequences of word indices\n","    sequences_indices = [[word_to_index[word] for word in sequence.split()] for sequence in sequences]\n","\n","    # Determine the maximum sequence length\n","    max_length = max(len(sequence) for sequence in sequences_indices)\n","\n","    # Pad the sequences to the same length\n","    sequences_padded = [sequence + [word_to_index['<PAD>']] * (max_length - len(sequence)) for sequence in sequences_indices]\n","\n","    return sequences_padded\n","############################################################################################"]},{"cell_type":"markdown","metadata":{"id":"CjFQLZU24YdF"},"source":["Creating The Model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16241,"status":"ok","timestamp":1679086469884,"user":{"displayName":"Nico Cereghini","userId":"07571101354927414086"},"user_tz":420},"id":"_9-rEE9V2EVh","outputId":"f81c55bf-14cc-4a2b-89d2-10a013589650"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 7, 10])\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from utils import translate_sentence, bleu, save_checkpoint, load_checkpoint\n","from torch.utils.tensorboard import SummaryWriter\n","import pandas as pd\n","\n","\n","class SelfAttention(nn.Module):\n","    def __init__(self, embed_size, heads):\n","        super(SelfAttention, self).__init__()\n","        self.embed_size = embed_size\n","        self.heads = heads\n","        self.head_dim = embed_size // heads\n","\n","        assert (self.head_dim * heads == embed_size), \"Embed size needs to be divisible by heads\"\n","\n","        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n","\n","    def forward(self, values, keys, query, mask):\n","        N = query.shape[0]\n","        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n","\n","        # split embedding into self.heads different pieces\n","        values = values.reshape(N, value_len, self.heads, self.head_dim)\n","        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n","        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n","\n","        valkeys = self.values(values)\n","        keys = self.keys(keys)\n","        queries = self.queries(queries)\n","\n","        energy = torch.einsum(\"nqhd, nkhd->nhqk\", [queries, keys])\n","        # queries shape: (N, query_len, heads, head_dim)\n","        # keys shape: (N, key_len, heads, head_dim)\n","        # energy: (N, heads, query_len, key_len)\n","        \n","        if mask is not None:\n","            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n","\n","        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3)\n","\n","        out = torch.einsum(\"nhql, nlhd->nqhd\", [attention, values]).reshape(\n","            N, query_len, self.heads * self.head_dim\n","        )\n","        # attention shape: (N, heads, query_len, key_len)\n","        # values shape: (N, value_len, heads, head_dim)\n","        # after einsum: (N, query_len, heads, head_dim) then flatten last two dimensions\n","\n","        out = self.fc_out(out)\n","        return out\n","\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, embed_size, heads, dropout, forward_expansion):\n","        super(TransformerBlock, self).__init__()\n","        self.attention = SelfAttention(embed_size, heads)\n","        self.norm1 = nn.LayerNorm(embed_size)\n","        self.norm2 = nn.LayerNorm(embed_size)\n","\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(embed_size, forward_expansion * embed_size),\n","            nn.ReLU(),\n","            nn.Linear(forward_expansion * embed_size, embed_size),\n","        )\n","\n","        self.dropout = nn.Dropout(dropout) \n","    \n","    def forward(self, value, key, query, mask):\n","        attention = self.attention(value, key, query, mask)\n","        x = self.dropout(self.norm1(attention + query))\n","        forward = self.feed_forward(x)\n","        out = self.dropout(self.norm2(forward + x))\n","        return out\n","\n","class Encoder(nn.Module):\n","    def __init__(\n","        self,\n","        src_vocab_size,\n","        embed_size,\n","        num_layers,\n","        heads,\n","        device,\n","        forward_expansion,\n","        dropout,\n","        max_length,\n","    ):\n","        super(Encoder, self).__init__()\n","        self.embed_size = embed_size\n","        self.device = device\n","        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n","        self.position_embedding = nn.Embedding(max_length, embed_size)\n","        self.layers = nn.ModuleList(\n","            [\n","                TransformerBlock(\n","                    embed_size, heads, dropout=dropout, forward_expansion=forward_expansion\n","                )\n","                for _ in range(num_layers)\n","            ]\n","        )\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask):\n","        N, seq_length = x.shape\n","        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n","        out = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n","        for layer in self.layers:\n","            out = layer(out, out, out, mask)\n","        return out\n","\n","class DecoderBlock(nn.Module):\n","    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n","        super(DecoderBlock, self).__init__()\n","        self.norm = nn.LayerNorm(embed_size)\n","        self.attention = SelfAttention(embed_size, heads)\n","        self.transformer_block = TransformerBlock(\n","            embed_size, heads, dropout, forward_expansion\n","        )\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, value, key, src_mask, trg_mask):\n","        attention = self.attention(x, x, x, trg_mask)\n","        query = self.dropout(self.norm(attention + x))\n","        out = self.transformer_block(value, key, query, src_mask)\n","        return out \n","\n","class Decoder(nn.Module):\n","    def __init__(\n","        self,\n","        trg_vocab_size,\n","        embed_size,\n","        num_layers,\n","        heads,\n","        forward_expansion,\n","        dropout,\n","        device,\n","        max_length,\n","    ):\n","        super(Decoder, self).__init__()\n","        self.device = device\n","        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n","        self.position_embedding = nn.Embedding(max_length, embed_size)\n","        self.layers = nn.ModuleList(\n","            [\n","                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n","                for _ in range(num_layers)\n","            ]\n","        )\n","        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, enc_out, src_mask, trg_mask):\n","        N, seq_length = x.shape\n","        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n","        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n","\n","        for layer in self.layers:\n","            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n","\n","        out = self.fc_out(x)\n","        return out\n","    \n","class Transformer(nn.Module):\n","    def __init__(\n","        self,\n","        src_vocab_size,\n","        trg_vocab_size,\n","        src_pad_idx,\n","        trg_pad_idx,\n","        embed_size=256,\n","        num_layers=6,\n","        forward_expansion=4,\n","        heads=8,\n","        dropout=0,\n","        device=\"cuda\",\n","        max_length=100,\n","    ):\n","        super(Transformer, self).__init__()\n","        self.encoder = Encoder(\n","            src_vocab_size,\n","            embed_size,\n","            num_layers,\n","            heads,\n","            device,\n","            forward_expansion,\n","            dropout,\n","            max_length,\n","        )\n","\n","        self.decoder = Decoder(\n","            trg_vocab_size,\n","            embed_size,\n","            num_layers,\n","            heads,\n","            forward_expansion,\n","            dropout,\n","            device,\n","            max_length,\n","        )\n","\n","        self.src_pad_idx = src_pad_idx\n","        self.trg_pad_idx = trg_pad_idx\n","        self.device = device\n","\n","    def make_src_mask(self, src):\n","        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n","        # (N, 1, 1, src_len)\n","        return src_mask.to(self.device)\n","\n","    def make_trg_mask(self, trg):\n","        N, trg_len = trg.shape\n","        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n","            N, 1, trg_len, trg_len\n","        )\n","\n","        return trg_mask.to(self.device)\n","\n","    def forward(self, src, trg):\n","        src_mask = self.make_src_mask(src)\n","        trg_mask = self.make_trg_mask(trg)\n","\n","        enc_src = self.encoder(src, src_mask)\n","        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n","        return out\n","\n","   #For Testing the model compilation\n","if __name__ == \"__main__\":\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)\n","\n","    trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n","\n","    src_pad_idx = 0\n","    trg_pad_idx = 0\n","    src_vocab_size = 10\n","    trg_vocab_size = 10\n","    model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx).to(device)\n","    out = model(x, trg[:, :-1])\n","    print(out.shape)\n","\n","\n","#print(val_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11087,"status":"ok","timestamp":1679086480966,"user":{"displayName":"Nico Cereghini","userId":"07571101354927414086"},"user_tz":420},"id":"kP9mmAJg6aoU","outputId":"8ff53680-5026-4a36-e816-720e9ff83a1c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Input data read. Example:  4. The ratification and implementation of the updated ILO conventions (vote)\tep-09-11-26-008-04.txt\t008-04\n"]}],"source":["#DATASET PREPARATION \n","from sklearn.model_selection import train_test_split\n","# Load the data from the file\n","file_path = '/content/drive/MyDrive/europarl-v10.de-en.tsv.gz'\n","input_data, target_data = read_dataset(file_path)\n","print(\"Input data read. Example: \", input_data[0])\n","# Split the data into training and validation sets\n","train_input, val_input, train_target, val_target = train_test_split(input_data, target_data, test_size=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1679086480967,"user":{"displayName":"Nico Cereghini","userId":"07571101354927414086"},"user_tz":420},"id":"zfTP5vCxEDH0","outputId":"4e2e548b-ec73-45fd-8357-dcb31e781c31"},"outputs":[{"output_type":"stream","name":"stdout","text":["800\n"]}],"source":["print(len(train_input))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mq9SM2jhEXy5"},"outputs":[],"source":["# Create a DataFrame with the training data\n","train_df = pd.DataFrame({'input': train_input, 'target': train_target})\n","\n","# Print the DataFrame\n","#print(train_df)\n","\n","# Create a DataFrame with the validation data\n","val_df = pd.DataFrame({'input': val_input, 'target': val_target})\n","\n","# Print the DataFrame (its long)\n","\n","#build vocab\n","vocab = build_vocab(train_input + val_input, train_target + val_target)\n","#word to index mappings\n","word_to_index = create_word_to_index_mapping(vocab)\n","index_to_word = create_index_to_word_mapping(vocab)\n","#text to sequence\n","train_input_sequences = sequence_to_sequence_of_word_indices(train_input, word_to_index)\n","train_target_sequences = sequence_to_sequence_of_word_indices(train_target, word_to_index)\n","val_input_sequences = sequence_to_sequence_of_word_indices(val_input, word_to_index)\n","val_target_sequences = sequence_to_sequence_of_word_indices(val_target, word_to_index)\n","#sequence to text is only necessary for sampling\n","# Convert a sequence of integer indices back into human-readable text\n","text = sequence_to_text([1, 2, 3], index_to_word)\n","#print(\"Demo of sequence to text: \",text)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1679086480968,"user":{"displayName":"Nico Cereghini","userId":"07571101354927414086"},"user_tz":420},"id":"USgQAlkqFHHQ","outputId":"9dbc9a30-e322-42a7-86eb-30a8f9b6d705"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([100])\n"]}],"source":["print(torch.tensor(train_input_sequences[0]).to(device).shape) #This is essentially what our input tensor will look like"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":180,"status":"ok","timestamp":1679086481141,"user":{"displayName":"Nico Cereghini","userId":"07571101354927414086"},"user_tz":420},"id":"Bsg407hx2q1u","outputId":"e640e527-208c-424a-a8e5-dbec842bae2a"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 76, 64])\n"]}],"source":["#Self attention test script\n","import torch\n","import torch.nn as nn\n","\n","# Define a dummy input tensor\n","batch_size = 2\n","seq_len = 76\n","embed_size = 64\n","heads = 8\n","x = torch.randn(batch_size, seq_len, embed_size, heads)\n","\n","# Initialize SelfAttention\n","selfattn = SelfAttention(embed_size, heads)\n","\n","# Generate dummy values, keys, and queries\n","values = keys = queries = torch.randn(batch_size, seq_len, embed_size)\n","\n","# Set mask to None\n","mask = None\n","\n","# Pass the input tensor through SelfAttention\n","output = selfattn(values, keys, queries, mask)\n","\n","# Print the output shape\n","print(output.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1679086481141,"user":{"displayName":"Nico Cereghini","userId":"07571101354927414086"},"user_tz":420},"id":"gVFVocj5IhCI","outputId":"dfe50828-7a16-4339-8e33-e0800dc07dd3"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 76, 64])\n"]}],"source":["#Encoder Test\n","import torch\n","\n","# define hyperparameters\n","src_vocab_size = 100\n","embed_size = 64\n","num_layers = 6\n","heads = 8\n","device = torch.device(\"cuda\")\n","forward_expansion = 4\n","dropout = 0.1\n","max_length = 100\n","\n","# create a dummy input tensor\n","batch_size = 1\n","seq_length = 76\n","dummy_input = torch.randint(low=0, high=src_vocab_size, size=(batch_size, seq_length)).to(device)\n","mask = torch.ones((batch_size, seq_length)).to(device)\n","\n","# create an instance of the encoder\n","encoder = Encoder(src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length).to(device)\n","\n","# pass the dummy input through the encoder and check the output shape\n","output = encoder(dummy_input, mask)\n","print(output.shape)  # expected output: torch.Size([16, 76, 512])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1679086481142,"user":{"displayName":"Nico Cereghini","userId":"07571101354927414086"},"user_tz":420},"id":"yCrdxStqK5vq","outputId":"94ece97f-099d-4be0-ac98-dff7fad94e3a"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([16, 76, 64])\n"]}],"source":["#Decoder Block Test\n","import torch\n","import torch.nn as nn\n","\n","# Define dummy input tensor\n","batch_size = 16\n","seq_length = 76\n","embed_size = 64\n","heads = 8\n","forward_expansion = 4\n","dropout = 0.2\n","device = 'cuda'\n","x = torch.randn(batch_size, seq_length, embed_size).to(device)\n","value = torch.randn(batch_size, seq_length, embed_size).to(device)\n","key = torch.randn(batch_size, seq_length, embed_size).to(device)\n","src_mask = torch.randn(batch_size, 1, seq_length, seq_length).to(device)\n","trg_mask = torch.randn(batch_size, 1, seq_length, seq_length).to(device)\n","\n","# Instantiate DecoderBlock\n","decoder_block = DecoderBlock(embed_size, heads, forward_expansion, dropout, device).to(device)\n","\n","# Pass input through DecoderBlock\n","out = decoder_block(x, value, key, src_mask, trg_mask)\n","\n","# Print output shape\n","print(out.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LGigxKm6DAvN"},"outputs":[],"source":["#Decoder Test\n","\n","import torch\n","import torch.nn as nn\n","\n","# Define hyperparameters\n","trg_vocab_size = 100\n","embed_size = 64\n","num_layers = 6\n","heads = 8\n","forward_expansion = 4\n","dropout = 0.1\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","max_length = 100\n","\n","# Create a dummy input tensor\n","x = torch.randint(0, trg_vocab_size, size=(64, 10)).to(device)\n","\n","# Create a dummy encoder output tensor\n","enc_out = torch.randn(64, 20, embed_size).to(device)\n","\n","# Create a dummy source and target mask\n","src_mask = torch.ones(8, 1, 20).to(device)\n","trg_mask = torch.ones(8, 10, 10).to(device) #Note: If the masks are not constructed properly, everything will break\n","\n","# Create a decoder instance\n","decoder = Decoder(\n","    trg_vocab_size,\n","    embed_size,\n","    num_layers,\n","    heads,\n","    forward_expansion,\n","    dropout,\n","    device,\n","    max_length\n",").to(device)\n","\n","# Pass the input tensor through the decoder\n","out = decoder(x, enc_out, src_mask, trg_mask)\n","\n","# Check the output shape\n","assert out.shape == torch.Size([64, 10, trg_vocab_size])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1679086481142,"user":{"displayName":"Nico Cereghini","userId":"07571101354927414086"},"user_tz":420},"id":"0S8y6kPJEvHj","outputId":"efb3682b-113d-4768-db73-9848248f1ba3"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([8, 10, 64])\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","device = torch.device(\"cuda\")\n","\n","# define input tensors\n","value = torch.randn(8, 10, 64).to(device)\n","key = torch.randn(8, 10, 64).to(device)\n","query = torch.randn(8, 10, 64).to(device)\n","mask = torch.ones(8, 1, 10, 10).to(device)\n","\n","# initialize transformer block\n","transformer_block = TransformerBlock(embed_size=64, heads=8, dropout=0.1, forward_expansion=4).to(device)\n","#Embed_size passed to the block must be the same as initialized in the input tensors above ^^^^^\n","\n","\n","# pass inputs through transformer block\n","output = transformer_block(value, key, query, mask)\n","\n","# check output shape\n","print(output.shape)  # should be torch.Size([8, 10, 512])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":179,"status":"ok","timestamp":1679086481317,"user":{"displayName":"Nico Cereghini","userId":"07571101354927414086"},"user_tz":420},"id":"j1tXhnW6Kjnc","outputId":"88c70e56-7c8b-46c2-9b16-1482f198ade2"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([8, 20])\n","torch.Size([8, 10])\n"]}],"source":["#Transformer test\n","import torch\n","import torch.nn as nn\n","\n","device = torch.device(\"cuda\")\n","# Initialize model parameters\n","src_vocab_size = 100\n","trg_vocab_size = 100\n","src_pad_idx = 0\n","trg_pad_idx = 0\n","embed_size = 64\n","num_layers = 6\n","forward_expansion = 4\n","heads = 8\n","dropout = 0\n","max_length = 100\n","\n","# Create a sample input\n","src = torch.randint(low=0, high=src_vocab_size, size=(8, 20)).to(device)\n","trg = torch.randint(low=0, high=trg_vocab_size, size=(8, 10)).to(device)\n","\n","# Initialize model\n","model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx,\n","                    embed_size, num_layers, forward_expansion, heads, dropout, device, max_length).to(device)\n","\n","print(src.shape)\n","print(trg.shape)\n","# Calculate the output\n","output = model(src, trg[:, :-1])\n","\n","# Check the shape of the output tensor\n","#assert output.shape == trg[:, 1:].shape + (trg_vocab_size)\n","\n","# Test encoder output\n","src_mask = model.make_src_mask(src)\n","enc_out = model.encoder(src, src_mask)\n","assert enc_out.shape == (8, 20, embed_size), f\"Expected encoder output shape (8, 20, 256), but got {enc_out.shape}\"\n","\n","# Test decoder output\n","trg_mask = model.make_trg_mask(trg)\n","dec_out = model.decoder(trg, enc_out, src_mask, trg_mask)\n","assert dec_out.shape == (8, 10, 100), f\"Expected decoder output shape (8, 10, 150), but got {dec_out.shape}\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1679086481317,"user":{"displayName":"Nico Cereghini","userId":"07571101354927414086"},"user_tz":420},"id":"5mEm9C7k4dmp","outputId":"bca1376b-e94c-493c-d451-aafd7c2d5fb5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device  cuda\n"]}],"source":["# We're ready to define everything we need for training our Seq2Seq model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device \", device)\n","\n","load_model = True\n","save_model = True\n","\n","# Training hyperparameters\n","num_epochs = 100\n","learning_rate = 1e-3\n","batch_size = 8\n","val_batch_size = batch_size\n","num_val_samples = len(val_input_sequences)\n","num_val_batches = (num_val_samples + val_batch_size - 1) // val_batch_size\n","\n","# Model hyperparameters\n","src_vocab_size = len(vocab)\n","trg_vocab_size = len(vocab)\n","embed_size = 64\n","num_heads = 8\n","num_layers = 6\n","#num_decoder_layers = 6\n","dropout = 0.10\n","max_length = 100\n","forward_expansion = 4\n","# Set the source and target padding indices\n","src_pad_idx = word_to_index['<PAD>']\n","trg_pad_idx = word_to_index['<PAD>']\n","\n","# Tensorboard to get nice loss plot\n","writer = SummaryWriter(\"runs/loss_plot\")\n","step = 0\n","\n","model = Transformer(\n","    src_vocab_size,\n","    trg_vocab_size,\n","    src_pad_idx,\n","    trg_pad_idx,\n","    embed_size,\n","    num_layers,\n","    forward_expansion,\n","    num_heads,\n","    dropout,\n","    device,\n","    max_length,\n",").to(device)\n","\n","# Set the criterion\n","criterion = torch.nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n","\n","# Create the optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1679071680489,"user":{"displayName":"Nico Cereghini","userId":"07571101354927414086"},"user_tz":420},"id":"ogM8BZOQ4peY","outputId":"d72e7784-f7c7-4d46-da68-d21f36683c36"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([63])\n","torch.Size([8, 63])\n"]}],"source":["#Debug\n","# Absolutely huge, run when you restart kernel\n","train_target_sequences = torch.tensor(train_target_sequences)\n","train_input_sequences = torch.tensor(train_input_sequences)\n","test = train_target_sequences[0]\n","print(test.shape)\n","#test = test.unsqueeze(0)\n","test  = test.repeat(8, 1)\n","print(test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"C4k9FNBL48E0","executionInfo":{"status":"ok","timestamp":1679090925192,"user_tz":420,"elapsed":244104,"user":{"displayName":"Nico Cereghini","userId":"07571101354927414086"}},"outputId":"95723188-d92a-4ced-f6dc-edc65cefd953"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training the model...\n","Epoch:  0 Validation Loss:  7.8590722131729125\n","Epoch:  0 Loss:  7.277273178100586\n","Epoch:  1 Loss:  6.944750785827637\n","Epoch:  2 Loss:  6.748601913452148\n","Epoch:  3 Loss:  6.585626602172852\n","Epoch:  4 Loss:  6.4092254638671875\n","Epoch:  5 Loss:  6.279175758361816\n","Epoch:  6 Loss:  6.269858360290527\n","Epoch:  7 Loss:  6.200812816619873\n","Epoch:  8 Loss:  6.233091831207275\n","Epoch:  9 Loss:  6.186375141143799\n","Epoch:  10 Validation Loss:  9.561073586940765\n","Epoch:  10 Loss:  6.195732593536377\n","Epoch:  11 Loss:  6.187384605407715\n","Epoch:  12 Loss:  6.150946617126465\n","Epoch:  13 Loss:  6.211514472961426\n","Epoch:  14 Loss:  6.2889275550842285\n","Epoch:  15 Loss:  6.743457794189453\n","Epoch:  16 Loss:  6.6832756996154785\n","Epoch:  17 Loss:  6.65725564956665\n","Epoch:  18 Loss:  6.653554916381836\n","Epoch:  19 Loss:  6.651622772216797\n","Epoch:  20 Validation Loss:  9.998440401554108\n","Epoch:  20 Loss:  6.3304524421691895\n","Epoch:  21 Loss:  6.448061943054199\n","Epoch:  22 Loss:  6.20059871673584\n","Epoch:  23 Loss:  6.625311851501465\n","Epoch:  24 Loss:  6.211893081665039\n","Epoch:  25 Loss:  6.600913047790527\n","Epoch:  26 Loss:  6.432322025299072\n","Epoch:  27 Loss:  6.3646464347839355\n","Epoch:  28 Loss:  6.2595906257629395\n","Epoch:  29 Loss:  6.166894912719727\n","Epoch:  30 Validation Loss:  10.870835440158844\n","Epoch:  30 Loss:  6.014612197875977\n","Epoch:  31 Loss:  6.086414337158203\n","Epoch:  32 Loss:  5.915337562561035\n","Epoch:  33 Loss:  6.903130054473877\n","Epoch:  34 Loss:  6.676529884338379\n","Epoch:  35 Loss:  6.705430507659912\n","Epoch:  36 Loss:  6.684443950653076\n","Epoch:  37 Loss:  6.629361629486084\n","Epoch:  38 Loss:  6.3748087882995605\n","Epoch:  39 Loss:  6.344142913818359\n","Epoch:  40 Validation Loss:  10.607191908359528\n","Epoch:  40 Loss:  6.3662872314453125\n","Epoch:  41 Loss:  6.339670658111572\n","Epoch:  42 Loss:  6.761508941650391\n","Epoch:  43 Loss:  6.802126407623291\n","Epoch:  44 Loss:  6.4599175453186035\n","Epoch:  45 Loss:  6.532839775085449\n","Epoch:  46 Loss:  6.446272850036621\n","Epoch:  47 Loss:  6.454923152923584\n","Epoch:  48 Loss:  6.640021324157715\n","Epoch:  49 Loss:  6.289680004119873\n","Epoch:  50 Validation Loss:  11.23491959810257\n","Epoch:  50 Loss:  6.267827987670898\n","Epoch:  51 Loss:  6.3366851806640625\n","Epoch:  52 Loss:  6.232255458831787\n","Epoch:  53 Loss:  6.324113368988037\n","Epoch:  54 Loss:  6.214932441711426\n","Epoch:  55 Loss:  6.091884613037109\n","Epoch:  56 Loss:  6.443925380706787\n","Epoch:  57 Loss:  6.44162130355835\n","Epoch:  58 Loss:  6.422199726104736\n","Epoch:  59 Loss:  6.336002349853516\n","Epoch:  60 Validation Loss:  10.354469845294952\n","Epoch:  60 Loss:  6.447824954986572\n","Epoch:  61 Loss:  6.311197280883789\n","Epoch:  62 Loss:  6.253572940826416\n","Epoch:  63 Loss:  6.234838485717773\n","Epoch:  64 Loss:  6.194005489349365\n","Epoch:  65 Loss:  6.131596088409424\n","Epoch:  66 Loss:  6.18265438079834\n","Epoch:  67 Loss:  6.09649133682251\n","Epoch:  68 Loss:  6.402661323547363\n","Epoch:  69 Loss:  6.189000606536865\n","Epoch:  70 Validation Loss:  10.83274466753006\n","Epoch:  70 Loss:  6.23639440536499\n","Epoch:  71 Loss:  6.3204026222229\n","Epoch:  72 Loss:  6.2418437004089355\n","Epoch:  73 Loss:  6.267862796783447\n","Epoch:  74 Loss:  6.1909027099609375\n","Epoch:  75 Loss:  6.119606971740723\n","Epoch:  76 Loss:  6.176982879638672\n","Epoch:  77 Loss:  6.18201208114624\n","Epoch:  78 Loss:  6.210809707641602\n","Epoch:  79 Loss:  6.200102806091309\n","Epoch:  80 Validation Loss:  10.79224935054779\n","Epoch:  80 Loss:  6.244170188903809\n","Epoch:  81 Loss:  6.15484619140625\n","Epoch:  82 Loss:  6.211232662200928\n","Epoch:  83 Loss:  6.2147040367126465\n","Epoch:  84 Loss:  7.0431904792785645\n","Epoch:  85 Loss:  6.7060227394104\n","Epoch:  86 Loss:  6.757433891296387\n","Epoch:  87 Loss:  6.828723907470703\n","Epoch:  88 Loss:  6.831706523895264\n","Epoch:  89 Loss:  6.836961269378662\n","Epoch:  90 Validation Loss:  9.506066017150879\n","Epoch:  90 Loss:  6.775136470794678\n","Epoch:  91 Loss:  6.745795726776123\n","Epoch:  92 Loss:  6.701493740081787\n","Epoch:  93 Loss:  6.694007396697998\n","Epoch:  94 Loss:  6.738365650177002\n","Epoch:  95 Loss:  6.596226215362549\n","Epoch:  96 Loss:  6.702536582946777\n","Epoch:  97 Loss:  6.635740756988525\n","Epoch:  98 Loss:  6.6842474937438965\n","Epoch:  99 Loss:  6.668076515197754\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["import matplotlib.pyplot as plt\n","print('Training the model...')\n","\n","# Lists to store training and validation losses\n","train_losses = []\n","val_losses = []\n","\n","val_every = 10\n","for epoch in range(num_epochs):\n","    # Loop over the training data\n","    for input_seq, target_seq in zip(train_input_sequences, train_target_sequences):\n","        # Convert the input and target sequences to tensors and move them to the device\n","        input_tensor = torch.tensor(input_seq).to(device)\n","        target_tensor = torch.tensor(target_seq).to(device)\n","        input_tensor = input_tensor.repeat(batch_size, 1)\n","        target_tensor = target_tensor.repeat(batch_size, 1)\n","\n","        # Forward pass\n","        output = model(input_tensor, target_tensor)\n","\n","        # Calculate the loss\n","        loss = criterion(output.view(-1, src_vocab_size), target_tensor.view(-1))\n","\n","        # Backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Compute validation loss after each epoch\n","    val_loss = 0.0\n","    num_val_batches = 0\n","    # Loop over the validation data\n","    if epoch % val_every == 0:\n","      for val_input_seq, val_target_seq in zip(val_input_sequences, val_target_sequences):\n","          # Convert the validation input and target sequences to tensors and move them to the device\n","          val_input_tensor = torch.tensor(val_input_seq).to(device)\n","          val_target_tensor = torch.tensor(val_target_seq).to(device)\n","          val_input_tensor = val_input_tensor.repeat(val_batch_size, 1)\n","          val_target_tensor = val_target_tensor.repeat(val_batch_size, 1)\n","\n","          # Disable gradient calculation during validation\n","          with torch.no_grad():\n","              # Forward pass on the validation data\n","              val_output = model(val_input_tensor, val_target_tensor)\n","\n","              # Calculate the validation loss\n","              val_loss += criterion(val_output.view(-1, trg_vocab_size), val_target_tensor.view(-1)).item()\n","              num_val_batches += 1\n","\n","      avg_val_loss = val_loss / num_val_batches\n","      print(\"Epoch: \", epoch, \"Validation Loss: \", avg_val_loss)\n","    # Append the losses to the lists for plotting\n","    train_losses.append(loss.item())\n","    val_losses.append(avg_val_loss)\n","    print(\"Epoch: \", epoch, \"Loss: \", loss.item())\n","\n","# Plot the losses\n","plt.plot(train_losses, label='Training Loss')\n","plt.plot(val_losses, label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"elapsed":157,"status":"error","timestamp":1678730432236,"user":{"displayName":"Nico Cereghini","userId":"07571101354927414086"},"user_tz":420},"id":"-gowpa-EcM90","outputId":"b16520d0-7692-4d4c-f280-6a9fb45311e7"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-35660506fa22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define a function to sample from the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_field\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_field\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext.legacy'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}],"source":["# Define a function to sample from the model\n","def sample(model, sentence, src_field, trg_field, max_len=50):\n","    model.eval()\n","\n","    # Tokenize the input sentence and convert to indices\n","    tokens = src_field.tokenize(sentence)\n","    indices = [src_field.vocab.stoi[token] for token in tokens]\n","    indices = torch.LongTensor(indices).unsqueeze(1).to(model.device)\n","\n","    # Forward pass through the model\n","    with torch.no_grad():\n","        encoder_outputs, encoder_mask = model.encoder(indices)\n","        decoder_input = torch.LongTensor([[trg_field.vocab.stoi['<sos>']]]).to(model.device)\n","\n","        for i in range(max_len):\n","            decoder_output, _ = model.decoder(decoder_input, encoder_outputs, encoder_mask)\n","            pred_token = decoder_output.argmax(dim=2)[-1].item()\n","            decoder_input = torch.cat([decoder_input, torch.LongTensor([[pred_token]])], dim=0)\n","            if pred_token == trg_field.vocab.stoi['<eos>']:\n","                break\n","\n","    # Convert the output indices to tokens and return the sentence\n","    tokens = [trg_field.vocab.itos[i] for i in decoder_input.squeeze().tolist()]\n","    return ' '.join(tokens[1:-1])\n","\n","# Example usage\n","# Initialize the SRC and TRG variables\n","SRC = Field(tokenize=lambda text: text.split(), init_token='<sos>', eos_token='<eos>', pad_token='<PAD>')\n","TRG = Field(tokenize=lambda text: text.split(), init_token='<sos>', eos_token='<eos>', pad_token='<PAD>')\n","\n","# Build the vocabularies for the fields\n","SRC.build_vocab(input_data, min_freq=2, specials=[SRC.init_token, SRC.eos_token, SRC.pad_token])\n","TRG.build_vocab(target_data, min_freq=2, specials=[TRG.init_token, TRG.eos_token, TRG.pad_token])\n","\n","# Convert the sequences to sequences of word indices\n","train_input_sequences = sequence_to_sequence_of_word_indices(input_data, SRC.vocab.stoi)\n","train_target_sequences = sequence_to_sequence_of_word_indices(target_data, TRG.vocab.stoi)\n","sentence = \"This is a test sentence.\"\n","translation = sample(model, sentence, SRC, TRG)\n","print(f\"Input: {sentence}\")\n","print(f\"Translation: {translation}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FevTkQ-OfFk_"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"mount_file_id":"1UCB7Cc8PKvzI25zP0E2CnUhI4oIzNk33","authorship_tag":"ABX9TyN92VbLazwCdKs+szLFbuGa"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}