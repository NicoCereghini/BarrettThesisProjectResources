{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#Use this space for manually computing the scaled dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is how you'll do diagrams for tensors from here on out\n",
    "import pandas as pd\n",
    "#pd.DataFrame(input)\n",
    "\n",
    "def printTensor(tensor):\n",
    "    df = pd.DataFrame(tensor)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5898,  1.5792,  0.8666,  1.0245],\n",
       "        [ 2.2796,  0.9813,  1.4023, -0.6616],\n",
       "        [ 0.1588,  0.4028, -1.7044, -1.8284],\n",
       "        [ 0.4366, -1.0197, -0.4217,  1.4877],\n",
       "        [-0.8311, -0.1762, -1.8709,  2.2313]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([[-1.5898,1.5792,0.8666,1.0245],[2.2796,0.9813,1.4023,-0.6616],[0.1588,0.4028,-1.7044,-1.8284],[0.4366,-1.0197,-0.4217, 1.4877],[-0.8311,-0.1762,-1.8709,2.2313]], dtype=torch.float32)\n",
    "input\n",
    "#Paper example input from embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WQ matrix:\n",
      "          0         1         2         3         4         5         6   \\\n",
      "0  -0.470987  0.079266  0.174696 -0.535052 -0.012049  0.214860  0.068636   \n",
      "1   0.648801 -0.012278 -0.040407  0.191615  0.118053  0.371963 -0.025763   \n",
      "2   0.173569  0.121498 -0.344515  0.109018 -0.084727  0.547651 -0.370525   \n",
      "3  -0.326787  0.244349 -0.194039  0.019395  0.780593  0.242292  0.798896   \n",
      "4  -0.283382  0.002806 -0.918048 -0.164136 -0.034875  0.337081  0.147311   \n",
      "5   0.628372  0.204924  0.044404 -0.050840  0.233980 -0.442214 -0.155582   \n",
      "6   0.270670  0.028798 -0.188753  0.567402  0.346777  0.162149  0.493569   \n",
      "7  -0.485668  0.380601  0.668065  0.420029  0.020114 -0.086267 -0.316658   \n",
      "8   0.405937 -0.668566  0.195740  0.096004 -0.687596 -0.412855  0.152536   \n",
      "9  -0.371967 -0.094910  0.205214 -0.152070 -0.425743  0.346095  0.047952   \n",
      "10 -0.636151  0.142995 -0.367989 -0.007011 -0.034609 -0.059427 -0.011449   \n",
      "11  0.347692  0.343580 -0.240791 -0.123103  0.200509  0.173337 -0.224128   \n",
      "12  0.276966 -0.209294  0.351260  0.802753  0.061504 -0.401296 -0.072146   \n",
      "13 -0.530647  0.026544  0.511111 -0.040076 -0.302572 -0.350196  0.003148   \n",
      "14  0.171959  0.186760 -0.147014  0.083199  0.123115  0.289849  0.119113   \n",
      "15 -0.159282 -0.145329  0.418188 -0.115810 -0.538708  0.274326  0.309014   \n",
      "\n",
      "          7         8         9         10        11        12        13  \\\n",
      "0  -0.519826 -0.584675  0.318290 -0.230934 -0.227148  0.221382 -0.285079   \n",
      "1  -0.200612 -0.036125 -0.452037  0.544260 -0.218552 -0.408570  0.121423   \n",
      "2   0.154762 -0.188297  0.264585 -0.357084 -0.519917 -0.422487  0.007947   \n",
      "3   0.342760 -0.077283 -0.281148 -0.527806  0.003715 -0.508774 -0.329225   \n",
      "4   0.098028  0.182365  0.046033 -0.569870 -0.281597  0.037182 -0.046556   \n",
      "5  -0.481690 -0.387881  0.178286  0.211072 -0.363219  0.521660  0.117635   \n",
      "6   0.782370 -0.126748  0.285631  0.117235  0.118380 -0.231606  0.841753   \n",
      "7  -0.043335  0.277018  0.073189  0.136629  0.041589 -0.386585  0.797083   \n",
      "8  -0.146358  0.929970 -0.049427  0.351418 -0.590408  0.294644 -0.120990   \n",
      "9   0.208257 -0.278433 -0.058924 -0.126661  0.465012 -0.918622  0.289958   \n",
      "10 -0.494763  0.159455 -0.078402  0.385852 -0.014380 -0.065897 -0.174609   \n",
      "11 -0.455079  0.737734 -0.113674 -0.397801  0.339428 -0.708360  0.085819   \n",
      "12  0.039347  0.172569  0.681900  0.022328  0.289383  0.528313 -0.355090   \n",
      "13 -0.547099 -0.313873 -0.127747  0.210711  0.196905  0.147024  0.613388   \n",
      "14 -0.125770 -0.065073  0.342308  0.081383  0.406029 -0.150531 -0.032137   \n",
      "15 -0.325563 -0.170107 -0.154072  0.100568  0.049257  0.490209 -0.500757   \n",
      "\n",
      "          14        15  \n",
      "0  -0.256408  0.088620  \n",
      "1  -0.084009 -0.462359  \n",
      "2  -0.218029 -0.452898  \n",
      "3  -0.874594 -0.154946  \n",
      "4   0.192762 -0.326819  \n",
      "5  -0.368542 -0.402426  \n",
      "6   0.176933 -0.583398  \n",
      "7  -0.739582 -0.152267  \n",
      "8   0.460255 -0.563785  \n",
      "9   0.138438 -0.270376  \n",
      "10  0.544405  0.002593  \n",
      "11 -0.106441 -0.277088  \n",
      "12  0.185134 -0.216410  \n",
      "13  1.108166  0.074203  \n",
      "14 -0.346241  0.133151  \n",
      "15 -0.231802  0.099788  \n",
      "\n",
      "WK matrix:\n",
      "          0         1         2         3         4         5         6   \\\n",
      "0   0.298301  0.142341  0.218733 -0.244475 -0.281001 -0.114292  0.193395   \n",
      "1   0.043667 -0.007142 -0.057813  0.406424 -0.904554 -0.135982  0.259916   \n",
      "2  -0.528144 -0.420408 -0.223012  0.958372  0.164531 -0.069729 -0.186352   \n",
      "3   0.001012 -0.585677 -0.144789  0.568687 -0.101128 -0.624695  0.452775   \n",
      "4  -0.166386  0.039728  0.102770  0.372507  0.387766  0.079088  0.310735   \n",
      "5   0.326123 -0.093051  0.140814  0.099740 -0.101717 -0.297099  0.072436   \n",
      "6  -0.676220  0.227287  0.173074 -0.363480  0.269986  0.240300 -0.519316   \n",
      "7  -0.099182  0.386808  0.194991 -0.242140 -0.181357 -0.267033 -0.319931   \n",
      "8  -0.144106 -0.460436 -0.111174 -0.093108  0.202744 -0.495104  0.656608   \n",
      "9  -0.297741  0.064688 -0.207697  0.647035  0.459903 -0.170101  0.317523   \n",
      "10 -0.315839  0.035871  0.135871 -0.303326  0.524229 -0.087706 -0.082875   \n",
      "11 -0.311193 -0.418680 -0.146706 -0.546191 -0.265799  0.063364  0.350502   \n",
      "12  0.936703 -0.483398 -0.245773 -0.469775 -0.143346 -0.137327 -0.572179   \n",
      "13 -0.245128 -0.552677 -0.236460  0.274149  0.161295 -0.430530  0.370472   \n",
      "14  0.058728 -0.313995 -0.368091  0.102606  0.778884  0.190947  0.457965   \n",
      "15 -0.008971  0.207082 -0.088784  0.091546 -0.270790  0.309597 -0.549512   \n",
      "\n",
      "          7         8         9         10        11        12        13  \\\n",
      "0   0.281177 -0.166216 -0.075789  0.292267  0.587813  0.073929  0.063757   \n",
      "1  -0.005480  0.317906 -0.489759 -0.276622 -0.048005 -0.196753  0.483771   \n",
      "2  -0.894343 -0.240802 -0.545601 -0.273855  0.052380 -0.107405  0.275302   \n",
      "3  -0.159453 -0.130010  0.710507  0.152105  0.192565  0.115355 -0.260078   \n",
      "4   0.823948 -0.584289  0.030310 -0.063709 -0.623747 -0.115753  0.263933   \n",
      "5   0.337864  0.379778  0.295267  0.268621  0.083500  0.427357  0.403411   \n",
      "6   0.157809 -0.305924 -0.276665 -0.493653 -0.049048  0.006266 -0.094093   \n",
      "7   0.079045 -0.375251  0.295008  0.703283  0.210734 -0.106579 -0.258300   \n",
      "8  -0.024362 -0.497546  0.195009  0.012185  0.344727 -0.004106  0.282625   \n",
      "9  -0.143710 -0.113940  0.347472  0.086833  0.205463 -0.365159 -0.083579   \n",
      "10 -0.094515  0.213479  0.261513  0.554420  0.009487  0.457505  0.166497   \n",
      "11 -0.325637  0.418532 -0.225838  0.177321  0.342747  0.257964  0.342934   \n",
      "12  0.118927 -0.033434 -0.021257 -0.352506  0.076292  0.257726  0.034895   \n",
      "13  0.160434  0.191626  0.069684 -0.104492  0.060833 -0.247040  0.248322   \n",
      "14 -0.486937  0.589083 -0.417339  0.467235 -0.456328 -0.110133 -0.370373   \n",
      "15  0.672761 -0.780315  0.291857  0.180328 -0.417686  0.053638 -0.402651   \n",
      "\n",
      "          14        15  \n",
      "0   0.161430 -0.277524  \n",
      "1   0.097106 -0.020561  \n",
      "2  -0.134943  0.462974  \n",
      "3  -0.087451  0.376825  \n",
      "4   0.070090  0.426478  \n",
      "5   0.122399  0.123006  \n",
      "6   0.307275 -0.480050  \n",
      "7  -0.156896 -0.063693  \n",
      "8   0.142209  0.234019  \n",
      "9   0.099467  0.229823  \n",
      "10 -0.343213  0.274168  \n",
      "11 -0.095850 -0.421172  \n",
      "12  0.265007  0.262426  \n",
      "13  0.010558  0.238728  \n",
      "14  0.349016 -0.086736  \n",
      "15  0.289369  0.102406  \n",
      "\n",
      "WV matrix:\n",
      "          0         1         2         3         4         5         6   \\\n",
      "0  -0.505009 -0.079430  0.284525 -0.624995  0.336269 -0.011196  0.746120   \n",
      "1   0.492237  0.545778  0.332076 -0.432311 -0.282053 -0.322448  0.348734   \n",
      "2   0.118118  0.098549  0.284141  0.087517 -0.305947 -0.178226 -0.345470   \n",
      "3   0.255253 -0.600738  0.118259  0.090275  0.131381 -0.851580  0.190211   \n",
      "4  -0.056239  0.517712  0.256127 -0.371858 -0.389280  0.069806  0.003103   \n",
      "5   0.014767  0.033710 -0.011955 -0.082652 -0.197048  0.263039 -0.024484   \n",
      "6   0.390171  0.271375 -0.259723  0.193911 -0.281809  0.093771 -0.063991   \n",
      "7  -0.061153 -0.424533  0.447732 -0.551206  0.381981 -0.272132 -0.380452   \n",
      "8  -0.159547 -0.097501  0.165013 -0.660240  0.031758  0.208471  0.118610   \n",
      "9   0.986114  0.152882 -0.032679  0.156232 -0.167292  0.553246 -0.072457   \n",
      "10  0.223972 -0.424489 -0.627005 -0.117660 -0.605487 -0.213373  0.017409   \n",
      "11  0.756384  0.068632 -0.186137  0.241519  0.584390  0.096986  0.498836   \n",
      "12 -0.035608 -0.393351 -0.538740 -0.750254  0.234965 -0.015867 -0.674929   \n",
      "13 -0.700046 -0.152057 -0.377004  0.115848 -0.474644 -0.370345  0.183858   \n",
      "14  0.256847 -0.073227  0.089674 -0.553725 -0.107132  0.439602 -0.205531   \n",
      "15 -0.597444 -0.045944  0.046346 -0.081846  0.118328  0.319112 -0.065405   \n",
      "\n",
      "          7         8         9         10        11        12        13  \\\n",
      "0  -0.412101  0.144320 -0.645917 -0.083074 -0.306548 -0.102540  0.008540   \n",
      "1  -0.014591 -0.606568  0.216581 -0.080852 -0.317316  0.498202 -0.066887   \n",
      "2   0.136941  0.333127  0.113161 -0.227937 -0.068562  0.003360  0.182973   \n",
      "3   0.766244 -0.111979  0.293589 -0.346235  0.146563  0.098305 -0.147052   \n",
      "4   0.455378 -0.321143 -0.125980 -0.158862  0.002211 -0.162651 -0.402557   \n",
      "5  -0.364566 -0.308322 -0.030053  0.049640 -0.373491 -0.537014  0.626140   \n",
      "6   0.122660 -0.807307 -0.032074  0.475350  0.055136 -0.130110  0.455846   \n",
      "7   0.444632 -0.669936  0.249499 -0.629883  0.349654 -0.061317 -0.626836   \n",
      "8  -0.697829  0.389275  0.089783  0.175309  0.340884 -0.007778  0.030814   \n",
      "9   0.360774  0.238551  0.088678 -0.376934  0.190629 -0.070020 -0.336225   \n",
      "10 -0.324823  0.007548 -0.630539  0.309717 -0.366870  0.049499 -0.050991   \n",
      "11 -0.509679  0.384251  0.255751  0.333744  0.506344 -0.396172 -0.051695   \n",
      "12  0.881294  0.273298 -0.602051  0.577542 -0.223421  0.527449  0.372699   \n",
      "13 -0.186014  0.079277 -0.182897  0.444221  0.049272 -0.296498  0.182829   \n",
      "14  0.130788  0.380032 -0.198468 -0.401649  0.206027 -0.179360 -0.292982   \n",
      "15 -0.072062  0.226008 -0.632852 -0.480882  0.141072 -0.029149 -0.006936   \n",
      "\n",
      "          14        15  \n",
      "0  -0.145221 -0.168364  \n",
      "1   0.261009  0.445190  \n",
      "2  -0.083945 -0.115601  \n",
      "3  -0.197385  0.145529  \n",
      "4  -0.009858  0.349089  \n",
      "5   0.178169 -0.183473  \n",
      "6   0.424592  0.333995  \n",
      "7   0.632727 -0.599402  \n",
      "8   0.046647 -0.100813  \n",
      "9   0.130781  0.572443  \n",
      "10  0.041086 -0.072966  \n",
      "11  0.696990  0.178982  \n",
      "12  0.160951  0.317493  \n",
      "13  0.272788  0.181393  \n",
      "14  0.095098  0.486103  \n",
      "15  0.588566  0.159758  \n"
     ]
    }
   ],
   "source": [
    "#Lets try to initialize the K,Q,V wieghts\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "d_model = 16\n",
    "input = nn.Linear(4, d_model)(input) # We need to reshape the input to be compatible for multiplication with our QKV weights\n",
    "input = input.reshape(1,5,d_model)\n",
    "# Our example input has a column dimension of 4, or 4 featues in the sequence. \n",
    "# This is important for the linear layers\n",
    "\n",
    "\n",
    "# Set dimensions of the weights\n",
    "W_Q = nn.Linear(d_model, d_model, bias=False)\n",
    "W_K = nn.Linear(d_model, d_model, bias=False)\n",
    "W_V = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "nn.init.kaiming_normal_(W_Q.weight)\n",
    "nn.init.kaiming_normal_(W_K.weight)\n",
    "nn.init.kaiming_normal_(W_V.weight)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"WQ matrix:\")\n",
    "    printTensor(W_Q.weight)\n",
    "    print(\"\\nWK matrix:\")\n",
    "    printTensor(W_K.weight)\n",
    "    print(\"\\nWV matrix:\")\n",
    "    printTensor(W_V.weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q matrix:\n",
      "         0         1         2         3         4         5         6   \\\n",
      "0  1.197957 -1.269360 -0.055508 -2.128647  0.974117 -0.342524 -2.896082   \n",
      "1  0.167170 -2.022422  1.035629  2.415249  3.104412 -0.894096  0.676284   \n",
      "2 -2.828585  1.758431 -0.259630 -1.649939  0.475646  0.315709  2.531099   \n",
      "3  1.219062 -0.773609  1.505667 -0.123691  0.331173  0.037534 -1.679208   \n",
      "4  1.114348  0.173057  0.744262 -2.361209 -0.294234  0.531684 -2.995847   \n",
      "\n",
      "         7         8         9         10        11        12        13  \\\n",
      "0 -1.332305  0.830155  0.478851  1.966173  0.563795 -0.038380  1.905199   \n",
      "1 -1.662630  1.382159 -1.976950 -0.751531 -0.135693  1.560863 -3.175181   \n",
      "2 -0.658981  1.895863  0.125164  1.259131  0.729176  0.542601  2.224545   \n",
      "3  0.163091 -1.108362  0.084641 -0.255893  0.493218 -0.360399 -1.593347   \n",
      "4  0.302378 -1.510241  0.732952  1.940135  1.477124 -1.475554  1.387943   \n",
      "\n",
      "         14        15  \n",
      "0 -0.844068  0.294967  \n",
      "1 -0.222186 -0.119694  \n",
      "2 -0.819916 -1.887909  \n",
      "3  0.624764 -0.143959  \n",
      "4  0.346938 -0.606899  \n",
      "\n",
      "K matrix:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(1, 5, 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m printTensor(Q\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m, :, :])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mK matrix:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mprintTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mV matrix:\u001b[39m\u001b[38;5;124m\"\u001b[39m,)\n\u001b[1;32m     12\u001b[0m printTensor(V)\n",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m, in \u001b[0;36mprintTensor\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprintTensor\u001b[39m(tensor):\n\u001b[0;32m----> 6\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:762\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    754\u001b[0m         mgr \u001b[39m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    755\u001b[0m             arrays,\n\u001b[1;32m    756\u001b[0m             columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    759\u001b[0m             typ\u001b[39m=\u001b[39mmanager,\n\u001b[1;32m    760\u001b[0m         )\n\u001b[1;32m    761\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 762\u001b[0m         mgr \u001b[39m=\u001b[39m ndarray_to_mgr(\n\u001b[1;32m    763\u001b[0m             data,\n\u001b[1;32m    764\u001b[0m             index,\n\u001b[1;32m    765\u001b[0m             columns,\n\u001b[1;32m    766\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    767\u001b[0m             copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    768\u001b[0m             typ\u001b[39m=\u001b[39;49mmanager,\n\u001b[1;32m    769\u001b[0m         )\n\u001b[1;32m    770\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    771\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    772\u001b[0m         {},\n\u001b[1;32m    773\u001b[0m         index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    776\u001b[0m         typ\u001b[39m=\u001b[39mmanager,\n\u001b[1;32m    777\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:329\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    324\u001b[0m         values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m    326\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m     \u001b[39m# by definition an array here\u001b[39;00m\n\u001b[1;32m    328\u001b[0m     \u001b[39m# the dtypes will be coerced to a single dtype\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     values \u001b[39m=\u001b[39m _prep_ndarraylike(values, copy\u001b[39m=\u001b[39;49mcopy_on_sanitize)\n\u001b[1;32m    331\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dtype_equal(values\u001b[39m.\u001b[39mdtype, dtype):\n\u001b[1;32m    332\u001b[0m     \u001b[39m# GH#40110 see similar check inside sanitize_array\u001b[39;00m\n\u001b[1;32m    333\u001b[0m     rcf \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m (is_integer_dtype(dtype) \u001b[39mand\u001b[39;00m values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:583\u001b[0m, in \u001b[0;36m_prep_ndarraylike\u001b[0;34m(values, copy)\u001b[0m\n\u001b[1;32m    581\u001b[0m     values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mreshape((values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m))\n\u001b[1;32m    582\u001b[0m \u001b[39melif\u001b[39;00m values\u001b[39m.\u001b[39mndim \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m--> 583\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMust pass 2-d input. shape=\u001b[39m\u001b[39m{\u001b[39;00mvalues\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    585\u001b[0m \u001b[39mreturn\u001b[39;00m values\n",
      "\u001b[0;31mValueError\u001b[0m: Must pass 2-d input. shape=(1, 5, 16)"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# Lets compute Q,K,V first linear projection\n",
    "with torch.no_grad():\n",
    "    Q = W_Q(input)\n",
    "    K = W_K(input)\n",
    "    V = W_V(input)\n",
    "    print(\"Q matrix:\")\n",
    "    printTensor(Q.numpy()[0, :, :]) # Changing print statements so I can see only the last two dimensions\n",
    "    print(\"\\nK matrix:\")\n",
    "    printTensor(K.numpy()[0, :, :])\n",
    "    print(\"\\nV matrix:\",)\n",
    "    printTensor(V.numpy()[0, :, :])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Dot Product:\n",
      "          0         1         2         3         4\n",
      "0  0.746611  0.760072  0.483585  0.555763  2.251897\n",
      "1  3.714852 -6.536989  1.316062 -0.318690  5.196238\n",
      "2 -0.382909  1.248583 -1.181069 -1.088685 -3.904110\n",
      "3 -1.515646  0.781833  0.291480 -0.311119 -1.935483\n",
      "4 -3.125703  4.380373 -0.287978 -1.128130 -6.200011\n"
     ]
    }
   ],
   "source": [
    "# Scaled dot-product    \n",
    "with torch.no_grad():\n",
    "    attention_weights = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_model)\n",
    "    print(\"Scaled Dot Product:\")\n",
    "    printTensor(attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax:\n",
      "          0         1         2         3         4\n",
      "0  0.123244  0.124914  0.094740  0.101832  0.555269\n",
      "1  0.181567  0.000006  0.016491  0.003216  0.798720\n",
      "2  0.141144  0.721459  0.063537  0.069687  0.004173\n",
      "3  0.047541  0.472992  0.289665  0.158559  0.031242\n",
      "4  0.000542  0.986179  0.009258  0.003996  0.000025\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    #softmax\n",
    "    attention_weights = torch.softmax(attention_weights, dim=-1)\n",
    "    print(\"Softmax:\")\n",
    "    printTensor(attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "          0         1         2         3\n",
      "0 -0.330422  0.354216  0.240682 -0.287233\n",
      "1 -0.704020  0.533120  0.187559 -1.103511\n",
      "2  1.005840  0.509922  1.177463 -0.027600\n",
      "3  0.636917  0.420271 -0.162054  1.025237\n",
      "4  1.426986  0.317784  1.805311  0.754292\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    #Last linear layer\n",
    "    print(\"Output:\")\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    printTensor(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9653,  1.3337,  0.5968, -0.9653],\n",
      "        [-0.6559,  1.2212,  0.6968, -1.2621],\n",
      "        [ 0.7226, -0.3331,  1.0880, -1.4775],\n",
      "        [ 1.2931,  0.6150, -1.2077, -0.7005],\n",
      "        [ 0.6077, -1.3133,  1.2630, -0.5573]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Applying layer normalization to a mini batch\n",
    "# Remember d_model is our embedding dimension\n",
    "dropout = nn.Dropout(p=0.1)\n",
    "layer_norm = nn.LayerNorm(output.size()[-1])\n",
    "# We now will call the norm layer with our output\n",
    "output = dropout(output)\n",
    "output = layer_norm(output)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3\n",
      "0  1.062668 -0.411011 -1.441476  0.789819\n",
      "1  0.320404 -0.892024  1.497872 -0.926251\n",
      "2  1.378798  0.523056 -1.039870 -0.861984\n",
      "3  1.616217 -0.982255 -0.650158  0.016196\n",
      "4  1.342489 -0.563447 -1.277423  0.498380\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    printTensor(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n",
      "torch.Size([5, 4])\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# a class for making multihead attetnion sublayer easier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiheadAttentionSublayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(embed_dim, embed_dim) for _ in range(3)])\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x, attn_mask):\n",
    "        q, k, v = [l(x) for l in self.linears]\n",
    "        x, attn = self.attention(q, k, v, attn_mask)\n",
    "        x = x + self.dropout(x)\n",
    "        x = self.layer_norm(x)\n",
    "        return x, attn\n",
    "\n",
    "    def attention(self, q, k, v, attn_mask):\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(k.size(-1))\n",
    "        if attn_mask is not None:\n",
    "            scores = scores.masked_fill(attn_mask, -1e9)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        x = torch.matmul(attn, v)\n",
    "        return x, attn\n",
    "        \n",
    "model = MultiheadAttentionSublayer(d_model, 6)\n",
    "attn_mask = None\n",
    "attention_weights, weighted_output = model(input, attn_mask) # Compute the output of the forward method\n",
    "print(weighted_output.shape) # Print the output tensor to verify it\n",
    "print(input.shape)\n",
    "print(d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do the last step in the encoder - feed forward\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the feedforward layer\n",
    "feedforward = nn.Sequential(\n",
    "    nn.Linear(output.size()[-1], 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, d_model)\n",
    ")\n",
    "\n",
    "# Pass the normalized tensor through the feedforward layer\n",
    "encoder_output = feedforward(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3\n",
      "0 -0.158562  0.339174 -0.396286 -0.223677\n",
      "1 -0.133566  0.335864 -0.438792 -0.219981\n",
      "2  0.209002  0.020946 -0.521161 -0.132638\n",
      "3  0.320134  0.019025 -0.788149 -0.231824\n",
      "4  0.412192 -0.240543 -0.453010  0.024202\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    printTensor(encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1782,  1.6422, -1.0477, -0.4164],\n",
      "        [-0.0688,  1.5921, -1.1488, -0.3746],\n",
      "        [ 1.0643,  0.2894, -1.6430,  0.2894],\n",
      "        [ 1.2054,  0.4652, -1.5191, -0.1515],\n",
      "        [ 1.4756, -0.5458, -1.2038,  0.2741]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Apply another norm layer to finishthe encoder!!!!\n",
    "# Applying layer normalization to a mini batch\n",
    "# Remember d_model is our embedding dimension\n",
    "dropout = nn.Dropout(p=0.1)\n",
    "layer_norm = nn.LayerNorm(encoder_output.size()[-1])\n",
    "# We now will call the norm layer with our output\n",
    "encoder_output = dropout(encoder_output)\n",
    "encoder_output = layer_norm(encoder_output)\n",
    "print(encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3\n",
      "0 -0.178205  1.642216 -1.047657 -0.416354\n",
      "1 -0.068807  1.592137 -1.148765 -0.374564\n",
      "2  1.064282  0.289353 -1.642988  0.289353\n",
      "3  1.205371  0.465171 -1.519063 -0.151480\n",
      "4  1.475616 -0.545838 -1.203828  0.274050\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    printTensor(encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep this for the decoder implementation\n",
    "\n",
    "def generate_attn_mask(input_seq, padding_idx):\n",
    "    attn_mask = (input_seq != padding_idx).unsqueeze(1).unsqueeze(2)\n",
    "    attn_mask = attn_mask.to(torch.float32).masked_fill(attn_mask == 0, float('-inf'))\n",
    "    return attn_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a tensor requires grads, use this method,\n",
    "# Convert the tensor to a numpy array\n",
    "tensor_numpy = tensor.cpu().numpy()\n",
    "\n",
    "# Create a DataFrame using the numpy array\n",
    "df = pd.DataFrame(tensor_numpy)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
