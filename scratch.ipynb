{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#Use this space for manually computing the scaled dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5898,  1.5792,  0.8666,  1.0245],\n",
       "        [ 2.2796,  0.9813,  1.4023, -0.6616],\n",
       "        [ 0.1588,  0.4028, -1.7044, -1.8284],\n",
       "        [ 0.4366, -1.0197, -0.4217,  1.4877],\n",
       "        [-0.8311, -0.1762, -1.8709,  2.2313]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([[-1.5898,1.5792,0.8666,1.0245],[2.2796,0.9813,1.4023,-0.6616],[0.1588,0.4028,-1.7044,-1.8284],[0.4366,-1.0197,-0.4217, 1.4877],[-0.8311,-0.1762,-1.8709,2.2313]], dtype=torch.float32)\n",
    "input\n",
    "#Paper example input from embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0167, -0.3836,  0.3057, -1.6176],\n",
      "        [-0.9095, -0.3848, -0.5388,  0.6638],\n",
      "        [ 0.3014, -0.3997, -0.4050,  0.3834],\n",
      "        [ 0.1564, -0.5142, -0.8222, -0.5973]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.4236,  0.4587,  0.4679,  1.3346],\n",
      "        [ 0.0984, -0.1086, -0.0687, -0.4526],\n",
      "        [ 0.1896, -0.1889, -0.5235,  0.0708],\n",
      "        [-0.8764, -0.2409, -0.3299, -0.5605]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0811, -0.1623,  1.5385,  0.6436],\n",
      "        [ 1.5437, -0.3447, -0.6894,  0.0305],\n",
      "        [ 0.2190,  0.6618,  0.5555,  0.0452],\n",
      "        [ 1.3458,  0.1445, -0.2622, -0.0180]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#Lets try to initialize the K,Q,V wieghts\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "# Our example input has a column dimension of 4, or 4 featues in the sequence. \n",
    "# This is important for the linear layers\n",
    "d_model = 4\n",
    "\n",
    "# Set dimensions of the weights\n",
    "W_Q = nn.Linear(d_model, d_model, bias=False)\n",
    "W_K = nn.Linear(d_model, d_model, bias=False)\n",
    "W_V = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "nn.init.kaiming_normal_(W_Q.weight)\n",
    "nn.init.kaiming_normal_(W_K.weight)\n",
    "nn.init.kaiming_normal_(W_V.weight)\n",
    "\n",
    "print(W_Q.weight)\n",
    "print(W_K.weight)\n",
    "print(W_V.weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q vector:\n",
      "          0         1         2         3\n",
      "0 -1.971605  1.051457 -1.068663 -2.385256\n",
      "1  1.084435 -3.645744 -0.526604 -0.905841\n",
      "2  2.279399 -0.594955 -0.123899  2.311266\n",
      "3 -2.151566  1.210121  1.280348  0.050730\n",
      "4 -4.099931  3.312943  1.432984  0.166065\n",
      "K vector:\n",
      "          0         1         2         3\n",
      "0  3.170568 -0.851225 -0.981001  0.152647\n",
      "1 -0.742425  0.320789 -0.533969 -2.326051\n",
      "2 -3.120222  0.916468  0.716774  1.350953\n",
      "3  1.135578 -0.490544  0.601512 -0.831683\n",
      "4  2.373835 -0.943862  1.012947  0.137440\n",
      "V vector:\n",
      "          0         1         2         3\n",
      "0  1.607486 -3.564692  1.224613 -2.157120\n",
      "1  1.757219  2.193852  1.897804  2.854013\n",
      "2 -3.851525  1.225622 -0.728067  0.751833\n",
      "3  0.509565  1.361481 -0.746195  0.523980\n",
      "4 -1.481155  0.135552 -1.237002 -0.693649\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "printTensor() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Scaled dot-product\u001b[39;00m\n\u001b[1;32m     14\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(Q, K\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(d_model)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mprintTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mScaled Dot Product:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mattention_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#softmax\u001b[39;00m\n\u001b[1;32m     17\u001b[0m attention_weights\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: printTensor() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# Lets compute Q,K,V first linear projection\n",
    "with torch.no_grad():\n",
    "    Q = W_Q(input)\n",
    "    K = W_K(input)\n",
    "    V = W_V(input)\n",
    "    print(\"Q vector:\")\n",
    "    printTensor(Q)\n",
    "    print(\"K vector:\")\n",
    "    printTensor(K)\n",
    "    print(\"V vector:\",)\n",
    "    printTensor(V)\n",
    "    # Scaled dot-product\n",
    "    attention_weights = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_model)\n",
    "    print(\"Scaled Dot Product:\")\n",
    "    printTensor(attention_weights)\n",
    "    #softmax\n",
    "    attention_weights.softmax(-1)\n",
    "    print(\"Softmax:\")\n",
    "    printTensor(attention_weights)\n",
    "    #Last linear layer\n",
    "    print(\"Output:\")\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    printTensor(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is how you'll do diagrams for tensors from here on out\n",
    "import pandas as pd\n",
    "#pd.DataFrame(input)\n",
    "\n",
    "def printTensor(tensor):\n",
    "    df = pd.DataFrame(tensor)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a tensor requires grads, use this method,\n",
    "# Convert the tensor to a numpy array\n",
    "tensor_numpy = tensor.cpu().numpy()\n",
    "\n",
    "# Create a DataFrame using the numpy array\n",
    "df = pd.DataFrame(tensor_numpy)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
