{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#Use this space for manually computing the scaled dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5898,  1.5792,  0.8666,  1.0245],\n",
       "        [ 2.2796,  0.9813,  1.4023, -0.6616],\n",
       "        [ 0.1588,  0.4028, -1.7044, -1.8284],\n",
       "        [ 0.4366, -1.0197, -0.4217,  1.4877],\n",
       "        [-0.8311, -0.1762, -1.8709,  2.2313]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([[-1.5898,1.5792,0.8666,1.0245],[2.2796,0.9813,1.4023,-0.6616],[0.1588,0.4028,-1.7044,-1.8284],[0.4366,-1.0197,-0.4217, 1.4877],[-0.8311,-0.1762,-1.8709,2.2313]], dtype=torch.float32)\n",
    "input\n",
    "#Paper example input from embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WQ matrix:\n",
      "          0         1         2         3\n",
      "0  0.280131  0.973659  0.650142  0.658475\n",
      "1  0.652911 -0.443674 -0.941379 -1.285408\n",
      "2  0.454926 -0.340234 -0.077881  0.398767\n",
      "3 -0.085917  0.376892  1.278757 -0.799545\n",
      "\n",
      "WK matrix:\n",
      "          0         1         2         3\n",
      "0 -1.256685 -0.824019 -0.450124 -0.223081\n",
      "1 -0.152469  0.114830  0.037177  0.903964\n",
      "2  0.079489 -0.151013 -0.610239  1.435603\n",
      "3 -0.242462  0.790393  0.287243 -0.669850\n",
      "\n",
      "WV matrix:\n",
      "          0         1         2         3\n",
      "0 -0.380946 -0.126559  0.535825 -0.000506\n",
      "1 -0.148489  0.819622  0.664562 -0.365945\n",
      "2  0.334110 -0.488277  0.150901  0.558298\n",
      "3  1.126758 -0.039417 -0.866881  0.231026\n"
     ]
    }
   ],
   "source": [
    "#Lets try to initialize the K,Q,V wieghts\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "# Our example input has a column dimension of 4, or 4 featues in the sequence. \n",
    "# This is important for the linear layers\n",
    "d_model = 4\n",
    "\n",
    "# Set dimensions of the weights\n",
    "W_Q = nn.Linear(d_model, d_model, bias=False)\n",
    "W_K = nn.Linear(d_model, d_model, bias=False)\n",
    "W_V = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "nn.init.kaiming_normal_(W_Q.weight)\n",
    "nn.init.kaiming_normal_(W_K.weight)\n",
    "nn.init.kaiming_normal_(W_V.weight)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"WQ matrix:\")\n",
    "    printTensor(W_Q.weight)\n",
    "    print(\"\\nWK matrix:\")\n",
    "    printTensor(W_K.weight)\n",
    "    print(\"\\nWV matrix:\")\n",
    "    printTensor(W_V.weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q matrix:\n",
      "          0         1         2         3\n",
      "0  2.330270 -3.871346 -0.919493  1.020816\n",
      "1  2.070086  0.583329  0.330141  2.496168\n",
      "2 -1.875383  3.879697 -0.661170 -0.579457\n",
      "3 -0.165087 -0.777848  1.171646 -2.150563\n",
      "4 -0.151472 -1.571365  0.717338 -4.171453\n",
      "\n",
      "K matrix:\n",
      "          0         1         2         3\n",
      "0  0.077962  1.382064  0.577092  1.196317\n",
      "1 -4.156968 -0.780814 -1.772519  1.068870\n",
      "2  0.643597 -1.694131 -1.632971  1.015044\n",
      "3  0.149523  1.145488  2.581776 -2.029489\n",
      "4  1.533999  2.053942  4.305502 -1.969797\n",
      "\n",
      "V matrix:\n",
      "          0         1         2         3\n",
      "0  0.869595  1.731415 -0.599508 -2.368121\n",
      "1 -0.240874  1.639825  0.124729  1.161404\n",
      "2 -1.023807 -0.157023 -1.421610  1.218156\n",
      "3 -0.263981 -1.725261  1.410713  1.241398\n",
      "4 -0.664702 -2.080871  0.771765  1.207833\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# Lets compute Q,K,V first linear projection\n",
    "with torch.no_grad():\n",
    "    Q = W_Q(input)\n",
    "    K = W_K(input)\n",
    "    V = W_V(input)\n",
    "    print(\"Q matrix:\")\n",
    "    printTensor(Q)\n",
    "    print(\"\\nK matrix:\")\n",
    "    printTensor(K)\n",
    "    print(\"\\nV matrix:\",)\n",
    "    printTensor(V)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Dot Product:\n",
      "          0         1         2         3         4\n",
      "0 -2.239093 -1.971559  5.298000 -4.265907 -5.173285\n",
      "1  2.072158 -3.488925  1.169339 -1.617938  0.439054\n",
      "2  2.070504  2.659580 -3.644104  1.816371  1.693290\n",
      "3 -1.492257 -1.540911 -1.442327  3.236886  3.714900\n",
      "4 -3.379974 -1.936814 -1.420495  4.247648  3.922783\n"
     ]
    }
   ],
   "source": [
    "# Scaled dot-product    \n",
    "with torch.no_grad():\n",
    "    attention_weights = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_model)\n",
    "    print(\"Scaled Dot Product:\")\n",
    "    printTensor(attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax:\n",
      "          0         1         2         3         4\n",
      "0  0.000532  0.000695  0.998674  0.000070  0.000028\n",
      "1  0.613662  0.002359  0.248794  0.015323  0.119862\n",
      "2  0.234358  0.422388  0.000773  0.181765  0.160716\n",
      "3  0.003347  0.003188  0.003518  0.378874  0.611073\n",
      "4  0.000282  0.001192  0.001998  0.578494  0.418034\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    #softmax\n",
    "    attention_weights = torch.softmax(attention_weights, dim=-1)\n",
    "    print(\"Softmax:\")\n",
    "    printTensor(attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "          0         1         2         3\n",
      "0 -1.022191 -0.154933 -1.419836  1.216209\n",
      "1  0.194635  0.751453 -0.607167 -0.983621\n",
      "2 -0.053548  0.450272  0.291540  0.356277\n",
      "3 -0.507656 -1.914749  0.999476  1.208470\n",
      "4 -0.432667 -1.865798  1.135852  1.226208\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    #Last linear layer\n",
    "    print(\"Output:\")\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    printTensor(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6697,  0.1882, -1.0631,  1.5446],\n",
       "        [ 0.5253,  1.3475, -0.6585, -1.2143],\n",
       "        [-1.6539,  0.9940,  0.1598,  0.5000],\n",
       "        [-0.3597, -1.4746,  0.8344,  1.0000],\n",
       "        [-0.3529, -1.4806,  0.8812,  0.9523]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying layer normalization to a mini batch\n",
    "# Remember d_model is our embedding dimension\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "# We now will call the norm layer with our output\n",
    "layer_norm(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is how you'll do diagrams for tensors from here on out\n",
    "import pandas as pd\n",
    "#pd.DataFrame(input)\n",
    "\n",
    "def printTensor(tensor):\n",
    "    df = pd.DataFrame(tensor)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a tensor requires grads, use this method,\n",
    "# Convert the tensor to a numpy array\n",
    "tensor_numpy = tensor.cpu().numpy()\n",
    "\n",
    "# Create a DataFrame using the numpy array\n",
    "df = pd.DataFrame(tensor_numpy)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
