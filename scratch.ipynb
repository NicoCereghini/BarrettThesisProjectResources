{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#Use this space for manually computing the scaled dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is how you'll do diagrams for tensors from here on out\n",
    "import pandas as pd\n",
    "#pd.DataFrame(input)\n",
    "\n",
    "def printTensor(tensor):\n",
    "    df = pd.DataFrame(tensor)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5898,  1.5792,  0.8666,  1.0245],\n",
       "        [ 2.2796,  0.9813,  1.4023, -0.6616],\n",
       "        [ 0.1588,  0.4028, -1.7044, -1.8284],\n",
       "        [ 0.4366, -1.0197, -0.4217,  1.4877],\n",
       "        [-0.8311, -0.1762, -1.8709,  2.2313]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([[-1.5898,1.5792,0.8666,1.0245],[2.2796,0.9813,1.4023,-0.6616],[0.1588,0.4028,-1.7044,-1.8284],[0.4366,-1.0197,-0.4217, 1.4877],[-0.8311,-0.1762,-1.8709,2.2313]], dtype=torch.float32)\n",
    "input\n",
    "#Paper example input from embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WQ matrix:\n",
      "          0         1         2         3         4         5         6    \\\n",
      "0    0.066657 -0.063596  0.001460 -0.044673  0.062044 -0.049699 -0.201590   \n",
      "1   -0.000749  0.009972  0.213238  0.018737  0.009318  0.069274 -0.094830   \n",
      "2   -0.014460 -0.103195  0.025643  0.138807 -0.090771  0.144126  0.211947   \n",
      "3    0.034983  0.044028  0.078618 -0.210516 -0.108374  0.126922  0.092009   \n",
      "4   -0.105210 -0.014070  0.121018 -0.204126 -0.205072 -0.182256  0.044908   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "123  0.023293 -0.023855  0.057618 -0.177262 -0.227320  0.098381 -0.078882   \n",
      "124  0.167402  0.092073  0.007225 -0.121935 -0.286025  0.232698  0.117591   \n",
      "125  0.024193  0.239837 -0.027919  0.306805 -0.005725  0.105184  0.109408   \n",
      "126 -0.055597  0.070930  0.026788  0.201177 -0.076031 -0.170954 -0.058242   \n",
      "127 -0.234322 -0.016151 -0.079854 -0.123666 -0.104125  0.133104  0.148430   \n",
      "\n",
      "          7         8         9    ...       118       119       120  \\\n",
      "0    0.076160  0.125724  0.101059  ... -0.053123 -0.130585 -0.067267   \n",
      "1   -0.129912 -0.100933 -0.223938  ... -0.216089  0.050350 -0.028354   \n",
      "2   -0.008908  0.084868  0.000531  ...  0.018044 -0.056546  0.192760   \n",
      "3    0.103856  0.162784  0.060144  ... -0.170343  0.157870 -0.004509   \n",
      "4    0.058479 -0.050305  0.099164  ... -0.109718  0.046714  0.001603   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "123 -0.083934  0.062887 -0.116781  ...  0.011427  0.129799 -0.097677   \n",
      "124  0.033487 -0.043998  0.134750  ... -0.010539 -0.007363  0.033107   \n",
      "125  0.263563 -0.204249 -0.065789  ...  0.068112 -0.149383  0.249878   \n",
      "126  0.078410  0.029029  0.082806  ...  0.005847  0.021992 -0.109669   \n",
      "127  0.113511  0.167112  0.070761  ... -0.040997  0.055085 -0.090065   \n",
      "\n",
      "          121       122       123       124       125       126       127  \n",
      "0   -0.124818 -0.035201 -0.051954 -0.111908 -0.021752 -0.052100 -0.121677  \n",
      "1    0.174364 -0.151057 -0.032352  0.052346  0.019727  0.168331 -0.072494  \n",
      "2    0.207588  0.196484 -0.125152  0.074803 -0.065214 -0.046633  0.029975  \n",
      "3    0.242205  0.004023  0.092813  0.063202  0.188329  0.216899  0.009045  \n",
      "4    0.060228  0.191745 -0.009043 -0.092125  0.044966 -0.042096  0.034803  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "123 -0.020706  0.062351 -0.034162 -0.074345 -0.044818  0.047304 -0.147104  \n",
      "124  0.054521  0.070894 -0.291568 -0.012417 -0.045948  0.209274  0.201595  \n",
      "125 -0.217775 -0.108161  0.195605  0.037933 -0.163719  0.013465  0.077987  \n",
      "126  0.110680 -0.020774 -0.122986 -0.044868 -0.079887  0.006716 -0.219259  \n",
      "127  0.240282 -0.018904  0.170785  0.193978 -0.143044 -0.202672 -0.006454  \n",
      "\n",
      "[128 rows x 128 columns]\n",
      "\n",
      "WK matrix:\n",
      "          0         1         2         3         4         5         6    \\\n",
      "0   -0.056001  0.007744 -0.123558 -0.117684 -0.090853 -0.136723  0.089959   \n",
      "1   -0.077621 -0.216515  0.074017 -0.139236  0.281015 -0.172682  0.038662   \n",
      "2   -0.123502  0.067141  0.043654 -0.030669 -0.013819  0.073485  0.053732   \n",
      "3   -0.093409 -0.253336 -0.015581  0.086131  0.056958  0.133343  0.288635   \n",
      "4   -0.024731 -0.027998 -0.128341  0.006917 -0.072102 -0.007384  0.040618   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "123 -0.032167  0.054092  0.092359 -0.167184 -0.008253 -0.241235  0.092217   \n",
      "124  0.271699  0.087838  0.086454 -0.168723 -0.043673 -0.124653  0.332918   \n",
      "125  0.120171  0.004370  0.098784  0.163242  0.108060  0.027139  0.020264   \n",
      "126 -0.117601  0.139567  0.103818 -0.103232 -0.084422  0.123784 -0.071233   \n",
      "127 -0.187592 -0.307792 -0.247159 -0.157548  0.033938  0.070210 -0.132727   \n",
      "\n",
      "          7         8         9    ...       118       119       120  \\\n",
      "0    0.106559  0.048700  0.080699  ... -0.088579  0.090590  0.119831   \n",
      "1    0.083590 -0.075438 -0.103935  ... -0.025118 -0.145936 -0.060435   \n",
      "2   -0.043247 -0.077350  0.005630  ... -0.156014 -0.038990 -0.022620   \n",
      "3   -0.056535 -0.261335 -0.148526  ... -0.031721  0.148506  0.034654   \n",
      "4   -0.062305  0.091110  0.074628  ... -0.085314 -0.041920  0.092101   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "123  0.043749  0.149229  0.064169  ... -0.109989 -0.032754 -0.029642   \n",
      "124 -0.038113 -0.177401 -0.021920  ...  0.040487 -0.053002 -0.083757   \n",
      "125 -0.029840  0.303309  0.085866  ...  0.027522 -0.077913 -0.148828   \n",
      "126  0.135146  0.169338  0.080594  ... -0.141410 -0.105740 -0.023752   \n",
      "127 -0.083775  0.020194 -0.018528  ... -0.044949  0.136172  0.191501   \n",
      "\n",
      "          121       122       123       124       125       126       127  \n",
      "0    0.070043  0.141395 -0.068093  0.205090 -0.128256 -0.008535  0.145090  \n",
      "1   -0.084654 -0.042243  0.157791  0.129558 -0.172816 -0.127861 -0.071693  \n",
      "2    0.322657 -0.042945  0.205801  0.091997 -0.085640 -0.159919  0.143389  \n",
      "3    0.010022  0.119907 -0.190438 -0.044676 -0.115246  0.081070  0.158421  \n",
      "4    0.161260  0.134941  0.236141 -0.270026 -0.227570  0.089090 -0.052592  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "123 -0.033815  0.054046 -0.013349  0.022108 -0.003020 -0.172838 -0.218651  \n",
      "124 -0.014295  0.080961  0.027198 -0.184226 -0.002504  0.000620 -0.149126  \n",
      "125 -0.010822  0.173054 -0.205521 -0.015148 -0.167771 -0.324465 -0.074091  \n",
      "126  0.126859 -0.090947 -0.081308 -0.080892 -0.059948  0.062988 -0.149879  \n",
      "127  0.054120  0.234091  0.186901 -0.027749 -0.069982 -0.214495 -0.055630  \n",
      "\n",
      "[128 rows x 128 columns]\n",
      "\n",
      "WV matrix:\n",
      "          0         1         2         3         4         5         6    \\\n",
      "0   -0.079763 -0.235306  0.082241  0.125889  0.258924 -0.062273  0.083042   \n",
      "1   -0.065043 -0.274818  0.024911  0.057677 -0.144498 -0.174119 -0.027648   \n",
      "2   -0.051829 -0.088525 -0.141646  0.109658  0.094865 -0.201098 -0.190400   \n",
      "3   -0.063438 -0.202668  0.202864 -0.007269 -0.061989 -0.025069  0.069323   \n",
      "4    0.023845  0.050230  0.100033  0.199594 -0.044277  0.120148  0.300298   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "123 -0.060086  0.083825 -0.066878 -0.209264  0.143932  0.190024  0.076831   \n",
      "124 -0.140828  0.097642 -0.002288  0.227149 -0.209695 -0.099670  0.021504   \n",
      "125  0.090495 -0.044943  0.167192 -0.248945 -0.079703  0.172098 -0.159672   \n",
      "126  0.135180 -0.086063 -0.223122 -0.018598  0.035316  0.128699  0.035532   \n",
      "127 -0.274428 -0.097748 -0.162141  0.023734 -0.173252 -0.093585 -0.014825   \n",
      "\n",
      "          7         8         9    ...       118       119       120  \\\n",
      "0   -0.106897 -0.205774 -0.150447  ... -0.110641 -0.103233  0.064303   \n",
      "1   -0.070229  0.151624 -0.137570  ... -0.021665  0.098520 -0.018417   \n",
      "2    0.006756  0.109514 -0.179848  ... -0.011206  0.040813 -0.057701   \n",
      "3    0.074886  0.096616  0.061134  ... -0.157530  0.153216 -0.204073   \n",
      "4    0.092086  0.073631  0.201474  ...  0.167657 -0.070142 -0.136825   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "123 -0.073993 -0.017853 -0.177337  ...  0.250593  0.114088 -0.191651   \n",
      "124  0.011661 -0.051002  0.193265  ...  0.088528  0.068396 -0.023676   \n",
      "125 -0.053637 -0.028729  0.017347  ... -0.093907 -0.085928 -0.161164   \n",
      "126 -0.208224 -0.222726  0.073413  ... -0.022871  0.216337  0.061932   \n",
      "127 -0.020795 -0.038189 -0.040117  ...  0.000022  0.045531  0.076938   \n",
      "\n",
      "          121       122       123       124       125       126       127  \n",
      "0   -0.042277 -0.053133 -0.003761  0.031400  0.056867  0.031975  0.265253  \n",
      "1    0.334128 -0.076248  0.090625  0.058846 -0.156452 -0.316742 -0.103258  \n",
      "2    0.160366 -0.114331 -0.308156  0.022823 -0.216441 -0.185927 -0.060416  \n",
      "3   -0.078441 -0.046387  0.070777  0.171440  0.220625  0.051249 -0.101777  \n",
      "4   -0.059723  0.006379 -0.107739  0.005955 -0.170561 -0.015676  0.096078  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "123  0.084911 -0.096684 -0.030567  0.175777 -0.099776 -0.044084  0.159045  \n",
      "124  0.066709  0.087494  0.056840  0.090591  0.086120  0.072815 -0.063734  \n",
      "125  0.084390 -0.124143 -0.049156 -0.129907 -0.143375  0.109956 -0.010473  \n",
      "126  0.116595  0.080147  0.167633 -0.324628 -0.124800 -0.042382  0.236427  \n",
      "127  0.065740 -0.013123  0.038730 -0.105116 -0.133821 -0.153632  0.033707  \n",
      "\n",
      "[128 rows x 128 columns]\n"
     ]
    }
   ],
   "source": [
    "#Lets try to initialize the K,Q,V wieghts\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "d_model = 128\n",
    "input = nn.Linear(4, d_model)(input) # We need to reshape the input to be compatible for multiplication with our QKV weights\n",
    "input = input.reshape(1,5,d_model)\n",
    "# Our example input has a column dimension of 4, or 4 featues in the sequence. \n",
    "# This is important for the linear layers\n",
    "\n",
    "\n",
    "# Set dimensions of the weights\n",
    "W_Q = nn.Linear(d_model, d_model, bias=False)\n",
    "W_K = nn.Linear(d_model, d_model, bias=False)\n",
    "W_V = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "nn.init.kaiming_normal_(W_Q.weight)\n",
    "nn.init.kaiming_normal_(W_K.weight)\n",
    "nn.init.kaiming_normal_(W_V.weight)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"WQ matrix:\")\n",
    "    printTensor(W_Q.weight)\n",
    "    print(\"\\nWK matrix:\")\n",
    "    printTensor(W_K.weight)\n",
    "    print(\"\\nWV matrix:\")\n",
    "    printTensor(W_V.weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q matrix:\n",
      "        0         1         2         3         4         5         6    \\\n",
      "0  1.053274 -1.486099  0.999819 -0.984640 -0.339790 -1.043898 -1.936681   \n",
      "1  0.374394  1.374941  2.334656  1.682233  0.727857  0.458205  1.111102   \n",
      "2 -0.359419  0.033485  0.756496 -0.123607 -0.112288 -0.572025  1.038903   \n",
      "3  0.724385  0.406115 -0.340476 -1.066427  0.281404  0.472168 -0.360281   \n",
      "4  1.068610 -0.872180 -0.827099 -2.542412 -0.904259 -0.369519 -1.903687   \n",
      "\n",
      "        7         8         9    ...       118       119       120       121  \\\n",
      "0  2.504586 -1.934478  0.092695  ...  0.010611 -1.136209 -2.275481 -1.574087   \n",
      "1  0.195757  0.154712 -0.018623  ... -1.660842  3.167106  1.809556  0.842297   \n",
      "2 -0.316079  1.548958  0.596741  ... -0.067625  0.607134 -1.017257  2.076622   \n",
      "3  0.579376 -0.865371 -0.582735  ... -1.846507  0.471877  0.630162 -1.252550   \n",
      "4  1.370767 -1.083481  0.160943  ... -0.610888 -1.295567 -1.226697 -1.319538   \n",
      "\n",
      "        122       123       124       125       126       127  \n",
      "0  0.511110  0.589320 -0.697104 -1.519361  0.892574  0.547651  \n",
      "1  0.801948  0.542404 -0.487841 -1.105196 -1.298072 -0.108374  \n",
      "2 -1.206541 -1.263008  1.461799  2.900476 -0.408958  0.065392  \n",
      "3  0.475868 -0.539238 -0.344100 -0.663849  0.864705 -0.157912  \n",
      "4 -0.095667 -1.217602  0.155045  0.244625  1.822784  0.155865  \n",
      "\n",
      "[5 rows x 128 columns]\n",
      "\n",
      "K matrix:\n",
      "        0         1         2         3         4         5         6    \\\n",
      "0  1.592520 -0.114901  1.115402 -1.442513 -1.265432  2.659557  0.066999   \n",
      "1  1.355872 -0.854272  1.528414 -0.509315  0.751551  0.108099 -0.368146   \n",
      "2 -0.863950  0.694918 -0.680714 -1.059463  0.789793  0.265717 -1.505307   \n",
      "3  1.162172  0.307716  0.674609  2.112754  0.265021 -1.158684  0.530822   \n",
      "4  0.865235  1.548017  0.162206  1.218997 -0.031690 -0.829000  0.104707   \n",
      "\n",
      "        7         8         9    ...       118       119       120       121  \\\n",
      "0 -1.645736  0.903485  0.095231  ... -0.509354 -0.175102  0.603700 -1.503022   \n",
      "1  0.264575  3.361464 -0.133040  ...  1.251257 -3.420492  0.539946 -1.567031   \n",
      "2  0.857384 -1.132747 -0.124393  ... -0.515194  0.256343 -0.162549  0.106612   \n",
      "3 -0.744272  0.434724  1.391724  ...  0.447030  0.178215  0.674394 -0.492593   \n",
      "4 -1.432333 -1.009008  1.435687  ... -0.431670  2.362277  0.820070 -0.169688   \n",
      "\n",
      "        122       123       124       125       126       127  \n",
      "0 -0.845095 -2.119813 -0.263157 -1.400003  1.365022  1.720408  \n",
      "1  0.527399  2.363757  0.499957 -2.344382 -1.173956 -0.133140  \n",
      "2  0.514155  1.377951  1.759246  0.184418 -2.176316 -1.416601  \n",
      "3 -0.440749 -2.383330 -0.059466 -0.420395 -0.414310 -1.137306  \n",
      "4 -1.084229 -3.810021 -0.114302  0.197858 -0.150904 -0.189419  \n",
      "\n",
      "[5 rows x 128 columns]\n",
      "\n",
      "V matrix:\n",
      "        0         1         2         3         4         5         6    \\\n",
      "0 -0.362250 -1.178739  2.465995 -0.485729  0.035758 -1.389083 -0.243294   \n",
      "1 -0.268425  1.342336 -1.813116 -1.002124 -2.302777 -2.986818  1.618111   \n",
      "2  1.417956  0.277213 -1.561006 -0.373527 -2.467091 -0.635563  2.481323   \n",
      "3 -0.371357  1.141985  1.199199 -0.413311  0.968422  0.181685 -0.587671   \n",
      "4 -0.262687  0.761734  2.735929  0.104726  0.837508  1.103542 -0.920182   \n",
      "\n",
      "        7         8         9    ...       118       119       120       121  \\\n",
      "0  0.812950 -0.333410 -1.354166  ... -1.320766  0.776757 -0.943758  0.980046   \n",
      "1  0.571790  2.006314  0.207718  ... -1.727640 -0.586182  0.478375 -0.569668   \n",
      "2  0.300975  0.385281 -0.518429  ...  0.271043 -1.550887 -0.696692  0.071543   \n",
      "3 -0.829551  0.429012  0.681629  ... -0.545447 -1.068721 -0.426894  1.501036   \n",
      "4 -0.743117  0.067135 -0.439439  ... -0.410413 -1.140947 -0.787465  2.833446   \n",
      "\n",
      "        122       123       124       125       126       127  \n",
      "0 -0.417541  0.518034  0.064662 -1.754187  2.802405  2.078211  \n",
      "1 -0.350936 -0.607039  1.260789 -1.787957 -0.660256  2.644420  \n",
      "2 -0.415578  0.170586 -0.610720  1.391863 -1.522178  0.837298  \n",
      "3  2.040515  1.307823  1.186044  0.707865  0.022206  0.327052  \n",
      "4  1.603325  2.616241  0.268079  2.299125  0.183650 -0.159559  \n",
      "\n",
      "[5 rows x 128 columns]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# Lets compute Q,K,V first linear projection\n",
    "with torch.no_grad():\n",
    "    Q = W_Q(input)\n",
    "    K = W_K(input)\n",
    "    V = W_V(input)\n",
    "    print(\"Q matrix:\")\n",
    "    printTensor(Q.numpy()[0, :, :]) # Changing print statements so I can see only the last two dimensions\n",
    "    print(\"\\nK matrix:\")\n",
    "    printTensor(K.numpy()[0, :, :])\n",
    "    print(\"\\nV matrix:\",)\n",
    "    printTensor(V.numpy()[0, :, :])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Dot Product:\n",
      "          0         1         2         3         4\n",
      "0  1.495476  3.379795 -0.158017 -0.624081 -1.742781\n",
      "1 -0.001339 -1.099216 -1.134368  2.743841  3.768891\n",
      "2  1.742415 -0.326832 -2.134887  2.027756  3.012411\n",
      "3  1.960517  1.379462  0.679449 -1.159631 -0.906804\n",
      "4  3.803500  2.940543 -0.342331 -1.745400 -2.191775\n"
     ]
    }
   ],
   "source": [
    "# Scaled dot-product    \n",
    "with torch.no_grad():\n",
    "    attention_weights = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_model)\n",
    "    print(\"Scaled Dot Product:\")\n",
    "    printTensor(attention_weights.numpy()[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax:\n",
      "          0         1         2         3         4\n",
      "0  0.126063  0.829728  0.024126  0.015138  0.004946\n",
      "1  0.016498  0.005503  0.005313  0.256833  0.715853\n",
      "2  0.165617  0.020914  0.003429  0.220306  0.589734\n",
      "3  0.515982  0.288593  0.143309  0.022781  0.029334\n",
      "4  0.692461  0.292158  0.010962  0.002695  0.001725\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    #softmax\n",
    "    attention_weights = torch.softmax(attention_weights, dim=-1)\n",
    "    print(\"Softmax:\")\n",
    "    printTensor(attention_weights.numpy()[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "        0         1         2         3         4         5         6    \\\n",
      "0 -0.241097  0.992921 -1.199499 -0.907472 -1.946889 -2.660482  1.358338   \n",
      "1 -0.283341  0.828002  2.288927 -0.046697  0.823063  0.793904 -0.791573   \n",
      "2 -0.297473  0.534612  2.242797 -0.131979  0.656556  0.396121 -0.670073   \n",
      "3 -0.077340 -0.132732  0.633025 -0.599707 -0.953043 -1.633289  0.656657   \n",
      "4 -0.315177 -0.416626  1.168727 -0.634154 -0.671003 -1.839082  0.328302   \n",
      "\n",
      "        7         8         9    ...       118       119       120       121  \\\n",
      "0  0.567941  1.638785 -0.002723  ... -1.603718 -0.447689  0.250783 -0.310660   \n",
      "1 -0.726860  0.165831 -0.163462  ... -0.463741 -1.089884 -0.689989  2.427258   \n",
      "2 -0.473367  0.122169 -0.330691  ... -0.616143 -0.797235 -0.707129  2.152309   \n",
      "3  0.586918  0.473932 -0.710438  ... -1.165699 -0.048447 -0.481574  0.468849   \n",
      "4  0.729771  0.360782 -0.881624  ... -1.418529  0.344768 -0.523900  0.521926   \n",
      "\n",
      "        122       123       124       125       126       127  \n",
      "0 -0.315024 -0.401519  1.058809 -1.648988 -0.230034  2.480495  \n",
      "1  1.660787  2.214847  0.501280  1.796253  0.171682  0.023065  \n",
      "2  1.317156  1.904691  0.454371  1.188676  0.558293  0.380318  \n",
      "3 -0.282760  0.223094  0.344581 -1.138087  1.043197  1.958245  \n",
      "4 -0.387951  0.191273  0.410090 -1.715942  1.731347  2.221453  \n",
      "\n",
      "[5 rows x 128 columns]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    #Last linear layer\n",
    "    print(\"Output:\")\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    printTensor(output.numpy()[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.9955e-01,  2.0107e-02, -1.0727e+00, -8.0666e-01, -1.7536e+00,\n",
      "          -2.4038e+00,  1.2576e+00,  5.3754e-01,  1.5132e+00,  2.0107e-02,\n",
      "          -1.9936e+00,  1.5892e+00,  1.2584e+00,  3.8098e-01,  2.3799e-01,\n",
      "           1.2612e-01,  3.9257e-01,  5.1634e-01,  2.0107e-02,  3.8195e-01,\n",
      "           3.2149e-01, -2.3478e-01,  8.5010e-01,  2.0107e-02,  3.8236e-01,\n",
      "           2.7220e-01, -8.3864e-01, -3.2977e+00,  9.6837e-01,  8.1118e-01,\n",
      "          -1.4694e-01,  8.6746e-01, -4.7537e-01, -2.5918e-01,  2.6218e-02,\n",
      "           1.1532e+00, -1.4637e+00,  9.5867e-03, -1.4466e+00, -6.2710e-01,\n",
      "          -8.3262e-02, -1.6121e-01, -7.6712e-02, -1.0796e+00,  3.0268e-01,\n",
      "           2.0107e-02, -1.0355e-01, -1.1874e-01,  2.0107e-02,  4.9191e-01,\n",
      "          -1.3745e+00,  4.2661e-01, -2.3794e-01,  4.1214e-01, -1.5925e-02,\n",
      "          -1.1717e+00,  2.4289e+00,  2.0107e-02, -1.3847e+00,  2.0107e-02,\n",
      "           2.0107e-02, -2.4601e-02,  8.3843e-01,  6.9493e-02, -1.1100e-01,\n",
      "           2.0107e-02, -1.4674e+00,  6.8193e-01, -3.5839e-01,  2.0107e-02,\n",
      "          -1.6302e+00,  3.5897e-01,  1.0353e+00, -1.2177e+00,  2.0107e-02,\n",
      "          -3.0915e-01,  5.3714e-01,  1.8852e-01,  9.3151e-02, -1.2610e-01,\n",
      "          -1.3506e+00, -1.3823e+00, -1.7650e-01,  7.7335e-01,  2.0107e-02,\n",
      "           1.4532e+00,  1.4059e+00,  3.0592e-01, -1.1958e+00,  7.7752e-01,\n",
      "          -1.6024e+00,  2.9304e-01,  1.6037e+00,  2.0107e-02,  2.7922e+00,\n",
      "          -7.7254e-01,  4.7650e-01, -2.3932e-01,  2.1816e+00,  1.7399e+00,\n",
      "           5.6266e-01, -1.1244e+00,  1.3971e+00,  1.0353e+00,  1.1211e+00,\n",
      "          -6.7941e-01, -5.4187e-01, -8.1886e-01,  2.0107e-02,  2.0107e-02,\n",
      "           2.0107e-02,  2.0107e-02,  2.0107e-02,  1.4000e+00, -2.0111e+00,\n",
      "          -1.2533e+00, -2.0642e+00,  8.1697e-01, -1.4410e+00,  2.0107e-02,\n",
      "           2.4859e-01, -2.6293e-01, -2.6690e-01, -3.4571e-01,  9.8476e-01,\n",
      "           2.0107e-02, -1.8947e-01,  2.2800e+00],\n",
      "         [-3.9405e-01,  6.5873e-01,  2.0427e+00, -1.2564e-01,  6.5405e-01,\n",
      "           6.2643e-01, -8.7550e-01, -8.1420e-01,  3.1452e-02, -2.8049e-01,\n",
      "           9.2092e-03, -6.3311e-01,  2.6268e-01, -1.4491e+00,  5.2015e-01,\n",
      "           3.4000e+00, -1.5429e+00, -4.7410e-01,  1.7346e+00,  6.3998e-01,\n",
      "          -5.9076e-01,  8.1530e-01, -1.6216e+00,  3.4476e-01, -9.0368e-01,\n",
      "           7.2640e-01,  1.2775e-01,  6.4916e-01,  1.3376e+00, -1.2564e-01,\n",
      "           2.0467e+00, -4.7406e-02, -3.4700e-01, -2.4431e-01,  9.3154e-01,\n",
      "          -2.8628e-01,  2.0040e-01, -1.2564e-01,  1.6209e+00, -8.0920e-01,\n",
      "          -1.2564e-01, -1.3735e-02, -5.7059e-01,  3.9920e-01, -9.7543e-01,\n",
      "          -7.0844e-01,  9.0802e-01,  1.5388e-01, -4.7154e-01, -3.6809e-01,\n",
      "          -6.8445e-01, -1.2874e+00, -7.9891e-03, -2.1424e+00, -1.2564e-01,\n",
      "          -5.3784e-01,  3.1437e-01, -1.3347e+00,  1.6309e+00, -4.9645e-01,\n",
      "           9.2650e-01, -2.2567e+00, -1.2564e-01,  8.3579e-02,  9.2328e-01,\n",
      "          -4.7911e-01, -2.5357e-01, -3.8644e-01, -2.3113e-01, -9.6728e-01,\n",
      "          -6.0228e-03,  9.9639e-01, -1.5692e+00,  1.9423e-01, -1.2564e-01,\n",
      "           1.7770e-01, -2.6296e-01, -3.2165e-02, -9.6081e-01, -1.0451e+00,\n",
      "           1.2321e+00,  2.7313e-01,  1.7722e-01, -1.9210e+00,  1.9980e+00,\n",
      "          -6.9161e-01, -4.4606e-01, -2.2667e+00,  3.7463e-01, -1.8316e+00,\n",
      "          -1.2564e-01, -1.2564e-01,  5.4259e-01, -9.3557e-01, -1.0135e+00,\n",
      "          -1.2564e-01, -1.7790e-02, -1.4969e-01, -1.9296e+00, -6.4487e-01,\n",
      "          -1.2113e-01, -3.6725e-02, -9.3705e-01, -4.9730e-01,  1.0622e+00,\n",
      "           1.6977e+00,  1.4666e-01,  8.3719e-01, -5.2195e-01,  7.2217e-01,\n",
      "           1.4656e+00,  9.0972e-02, -3.1089e-01, -5.5623e-01,  2.1603e+00,\n",
      "           6.3475e-01,  8.0861e-01,  1.8675e-01, -5.6494e-01, -1.1581e+00,\n",
      "          -7.7927e-01,  2.1737e+00,  1.4476e+00,  1.9725e+00,  3.4922e-01,\n",
      "           1.5760e+00,  3.6995e-02, -1.0379e-01],\n",
      "         [-4.0890e-01,  5.2775e-01,  2.4506e+00, -2.2261e-01,  6.6502e-01,\n",
      "           3.7185e-01, -8.2832e-01, -6.0690e-01,  6.3475e-02, -4.4629e-01,\n",
      "           6.5208e-02, -7.4046e-02,  2.5220e-01, -1.5149e+00,  3.8449e-01,\n",
      "           3.4266e+00, -1.2955e+00, -6.3485e-01,  2.2153e+00, -7.4046e-02,\n",
      "          -4.1448e-01,  9.4704e-01, -1.6723e+00,  3.8714e-01, -7.8665e-01,\n",
      "           4.3994e-01, -2.7088e-01,  7.5481e-01,  1.5066e+00,  9.5923e-01,\n",
      "          -7.4046e-02,  1.9583e-01, -7.4046e-02, -5.5630e-01,  1.1745e+00,\n",
      "          -2.5121e-01, -7.4046e-02, -6.3142e-01, -7.4046e-02, -7.4046e-02,\n",
      "          -1.2405e+00, -3.9542e-02, -7.4046e-02,  2.7881e-01, -8.2125e-01,\n",
      "          -5.8031e-01,  1.0793e+00, -6.8170e-02, -6.2087e-01,  3.5913e-02,\n",
      "          -1.0337e+00, -1.2785e+00,  5.4554e-02, -2.3391e+00,  5.0389e-01,\n",
      "          -5.5161e-01,  6.6585e-01, -1.2435e+00, -7.4046e-02, -7.4046e-02,\n",
      "           9.8947e-01, -2.6424e+00, -5.1604e-01, -2.3632e-01,  1.0180e+00,\n",
      "          -6.7415e-01, -8.6717e-02, -3.8539e-01,  1.2424e-01, -8.7582e-01,\n",
      "          -2.1309e-01, -7.4046e-02, -1.6399e+00,  2.0610e-01, -9.1544e-01,\n",
      "          -4.7025e-02,  1.0055e-01,  7.1328e-02, -9.3165e-01, -7.4046e-02,\n",
      "           1.1968e+00, -7.4046e-02,  2.3008e-01, -1.8092e+00,  2.4550e+00,\n",
      "          -6.1836e-01, -7.4046e-02, -2.1811e+00,  2.5418e-01, -1.8572e+00,\n",
      "          -7.4046e-02, -7.4046e-02,  9.6798e-01, -8.5154e-01, -8.2399e-01,\n",
      "           1.8139e+00,  1.1615e-01, -7.4046e-02, -1.8884e+00, -7.4046e-02,\n",
      "          -1.1561e-01, -3.1642e-01, -5.9574e-01, -3.4023e-01,  1.1298e+00,\n",
      "           1.9644e+00,  3.7658e-02,  1.0395e+00, -5.0836e-01,  5.9983e-01,\n",
      "           1.3953e+00,  4.1651e-01, -3.6175e-01, -5.0814e-01,  2.0252e+00,\n",
      "           4.2789e-01,  7.2996e-01,  3.6046e-01, -7.4046e-02, -9.7147e-01,\n",
      "          -8.7004e-01,  2.3487e+00,  1.4086e+00,  2.0700e+00,  4.3742e-01,\n",
      "          -7.4046e-02, -7.4046e-02,  3.5406e-01],\n",
      "         [-1.8873e-02, -9.8469e-02,  1.0019e+00, -7.6949e-01,  9.2260e-02,\n",
      "          -2.2547e+00,  9.2260e-02,  9.3563e-01,  7.7328e-01, -9.2860e-01,\n",
      "          -8.8943e-01,  1.4527e+00,  4.6604e-01, -5.5787e-01, -6.7829e-01,\n",
      "           1.3912e-01,  2.6959e-01, -3.6099e-01,  2.1647e+00, -1.1605e-01,\n",
      "           9.2260e-02, -3.7432e-02, -2.9021e-01, -4.7711e-01,  5.7523e-01,\n",
      "          -9.3407e-01, -1.8991e+00, -1.5197e+00,  1.6728e+00,  8.4030e-01,\n",
      "           8.9804e-01,  8.1510e-01, -2.2484e-02, -8.7450e-01,  1.2457e+00,\n",
      "           3.3263e-01, -2.4707e+00, -2.0779e-01, -3.8592e-01,  7.3443e-01,\n",
      "          -1.4114e+00, -3.2213e-01,  1.1186e+00, -7.9903e-01,  2.1198e-01,\n",
      "           1.7073e-01,  5.0661e-01, -1.0585e+00, -1.4701e+00,  1.4399e+00,\n",
      "          -2.1583e+00,  9.2260e-02, -1.6396e-01, -1.0968e+00, -5.1142e-01,\n",
      "          -6.6298e-01,  2.0577e+00,  5.3129e-01,  1.9955e-01, -5.0863e-01,\n",
      "           4.6538e-01, -2.0152e+00,  6.4619e-01, -1.5364e+00,  2.8080e-01,\n",
      "          -1.5751e+00,  9.2260e-02,  2.8761e-01,  6.2738e-01,  1.3565e-01,\n",
      "          -1.6348e+00,  1.0627e+00,  2.2195e-01, -7.2362e-01, -6.8492e-02,\n",
      "          -8.3270e-01,  1.3189e+00,  6.2539e-01,  1.5582e-01, -2.6730e-01,\n",
      "          -9.3082e-01, -9.2343e-01, -8.1649e-02,  3.6924e-01,  9.2752e-01,\n",
      "           9.2864e-01,  7.4558e-01,  2.3498e-01, -1.2395e+00, -5.8049e-02,\n",
      "          -9.9005e-01,  8.1322e-02,  2.1008e+00,  1.9081e-01,  1.6163e+00,\n",
      "           4.6555e-01,  3.1475e-01,  5.7689e-01,  9.2260e-02,  7.0669e-01,\n",
      "           5.5562e-01, -1.1211e+00,  1.5207e+00,  9.2260e-02,  9.8741e-01,\n",
      "           8.1218e-01, -5.8432e-01,  8.9138e-02, -1.1644e+00,  9.2260e-02,\n",
      "          -9.2922e-02,  2.0729e+00, -8.1189e-01,  5.5906e-01, -1.3310e+00,\n",
      "          -1.0443e+00, -1.0506e+00,  1.1313e+00, -1.5828e+00,  2.2644e-02,\n",
      "          -5.9974e-01,  7.6597e-01, -3.1405e-01,  9.2260e-02,  5.8741e-01,\n",
      "          -1.5431e+00,  1.5913e+00,  2.9062e+00],\n",
      "         [-2.7012e-01, -3.8855e-01,  1.4621e+00, -6.4249e-01, -6.8550e-01,\n",
      "          -2.0491e+00,  4.8105e-01,  9.4972e-01,  5.1897e-01, -9.3137e-01,\n",
      "          -5.3525e-01,  1.2424e+00,  2.1697e-01, -4.1248e-01, -6.6335e-01,\n",
      "           1.6428e-01,  9.5120e-01, -6.9979e-01,  2.2459e+00,  5.3153e-02,\n",
      "           7.1249e-01,  3.6980e-01, -2.0047e-01, -3.4436e-01,  4.8129e-01,\n",
      "          -1.3088e+00, -2.2083e+00, -7.1243e-01,  1.0741e+00,  8.8595e-01,\n",
      "           9.6309e-01,  9.7805e-02,  1.6654e-01, -1.5849e+00,  1.0346e+00,\n",
      "           3.5455e-01, -2.4471e+00, -1.0377e-01,  1.4063e-03,  1.3154e+00,\n",
      "          -1.8282e+00, -3.1560e-01,  1.1286e+00, -9.9459e-01,  5.6676e-01,\n",
      "           3.4707e-01,  6.4383e-01, -1.1873e+00, -1.4496e+00,  9.7805e-02,\n",
      "          -2.2140e+00, -3.8472e-02,  4.4234e-02, -1.0342e+00, -6.3062e-01,\n",
      "           9.7805e-02,  9.7805e-02,  4.8179e-01,  3.2227e-01, -7.9334e-01,\n",
      "           3.8987e-01, -2.0549e+00,  2.7132e-01, -1.6003e+00,  3.0044e-01,\n",
      "          -1.6135e+00,  9.7805e-02,  5.7848e-02,  1.3269e+00,  1.3042e-01,\n",
      "          -1.6122e+00,  1.1585e+00, -1.5211e-01, -4.8368e-01,  9.7805e-02,\n",
      "          -1.2213e+00,  9.7805e-02,  4.0995e-01, -1.1140e-02, -2.9112e-01,\n",
      "          -6.8630e-01, -8.9321e-01,  4.4623e-02,  5.1798e-01,  1.4924e+00,\n",
      "           6.4299e-01,  4.5996e-01,  2.4650e-01, -1.0541e+00, -1.1974e-01,\n",
      "          -8.6961e-01,  6.8309e-02,  2.3322e+00,  9.7805e-02,  1.5646e+00,\n",
      "           8.0293e-01,  5.9607e-01,  7.5232e-01,  7.0011e-01,  2.6887e-01,\n",
      "           9.7805e-02,  9.7805e-02,  1.7804e+00,  9.1117e-01,  6.1408e-01,\n",
      "           9.5569e-01, -7.7209e-01,  5.3092e-01, -8.9331e-01, -8.6861e-01,\n",
      "          -3.7895e-01,  1.8640e+00, -6.9526e-01,  4.8809e-01,  9.7805e-02,\n",
      "          -1.4167e+00, -1.1083e+00,  9.5324e-01, -1.5581e+00,  5.0028e-01,\n",
      "          -5.1378e-01,  7.0708e-01, -3.5508e-01,  3.2109e-01,  5.7653e-01,\n",
      "          -1.9053e+00,  2.1189e+00,  2.6911e+00]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Applying layer normalization to a mini batch\n",
    "# Remember d_model is our embedding dimension\n",
    "dropout = nn.Dropout(p=0.1)\n",
    "layer_norm = nn.LayerNorm(output.size()[-1])\n",
    "# We now will call the norm layer with our output\n",
    "output = dropout(output)\n",
    "output = layer_norm(output)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0         1         2         3         4         5         6    \\\n",
      "0 -0.199549  0.020107 -1.072721 -0.806664 -1.753645 -2.403779  1.257648   \n",
      "1 -0.394051  0.658730  2.042672 -0.125640  0.654051  0.626429 -0.875501   \n",
      "2 -0.408901  0.527748  2.450591 -0.222610  0.665016  0.371853 -0.828323   \n",
      "3 -0.018873 -0.098469  1.001886 -0.769489  0.092260 -2.254695  0.092260   \n",
      "4 -0.270122 -0.388551  1.462139 -0.642486 -0.685502 -2.049080  0.481054   \n",
      "\n",
      "        7         8         9    ...       118       119       120       121  \\\n",
      "0  0.537540  1.513155  0.020107  ... -1.440993  0.020107  0.248587 -0.262926   \n",
      "1 -0.814198  0.031452 -0.280489  ... -0.564945 -1.158093 -0.779270  2.173714   \n",
      "2 -0.606899  0.063475 -0.446294  ... -0.074046 -0.971465 -0.870036  2.348732   \n",
      "3  0.935632  0.773276 -0.928604  ... -1.582792  0.022644 -0.599738  0.765972   \n",
      "4  0.949717  0.518971 -0.931375  ... -1.558140  0.500276 -0.513779  0.707085   \n",
      "\n",
      "        122       123       124       125       126       127  \n",
      "0 -0.266902 -0.345706  0.984756  0.020107 -0.189470  2.280012  \n",
      "1  1.447632  1.972495  0.349225  1.575959  0.036995 -0.103791  \n",
      "2  1.408630  2.069997  0.437423 -0.074046 -0.074046  0.354064  \n",
      "3 -0.314053  0.092260  0.587406 -1.543114  1.591282  2.906160  \n",
      "4 -0.355076  0.321092  0.576531 -1.905330  2.118924  2.691058  \n",
      "\n",
      "[5 rows x 128 columns]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    printTensor(output.numpy()[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5])\n",
      "torch.Size([1, 5, 128])\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "# a class for making multihead attetnion sublayer easier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiheadAttentionSublayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(embed_dim, embed_dim) for _ in range(3)])\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x, attn_mask):\n",
    "        q, k, v = [l(x) for l in self.linears]\n",
    "        x, attn = self.attention(q, k, v, attn_mask)\n",
    "        x = x + self.dropout(x)\n",
    "        x = self.layer_norm(x)\n",
    "        return x, attn\n",
    "\n",
    "    def attention(self, q, k, v, attn_mask):\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(k.size(-1))\n",
    "        if attn_mask is not None:\n",
    "            scores = scores.masked_fill(attn_mask, -1e9)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        x = torch.matmul(attn, v)\n",
    "        return x, attn\n",
    "        \n",
    "model = MultiheadAttentionSublayer(d_model, 6)\n",
    "attn_mask = None\n",
    "attention_weights, weighted_output = model(input, attn_mask) # Compute the output of the forward method\n",
    "print(weighted_output.shape) # Print the output tensor to verify it\n",
    "print(input.shape)\n",
    "print(d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do the last step in the encoder - feed forward\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the feedforward layer\n",
    "feedforward = nn.Sequential(\n",
    "    nn.Linear(output.size()[-1], 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, d_model)\n",
    ")\n",
    "\n",
    "# Pass the normalized tensor through the feedforward layer\n",
    "encoder_output = feedforward(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0         1         2         3         4         5         6    \\\n",
      "0 -0.109593  0.218936 -0.121593 -0.131467 -0.242980 -0.466432  0.042785   \n",
      "1 -0.155166 -0.200968 -0.079917 -0.262690  0.072086 -0.512122 -0.169378   \n",
      "2 -0.121902 -0.125374 -0.261691 -0.311944  0.185182 -0.524246 -0.097686   \n",
      "3 -0.172394  0.298224 -0.157668 -0.315232 -0.073443 -0.325669 -0.099018   \n",
      "4 -0.354316  0.433445 -0.024942 -0.269588 -0.031008 -0.382633 -0.227058   \n",
      "\n",
      "        7         8         9    ...       118       119       120       121  \\\n",
      "0 -0.201919 -0.215250 -0.230467  ...  0.225229 -0.153292  0.333510  0.099333   \n",
      "1 -0.175118 -0.304439 -0.112535  ...  0.474310 -0.097140  0.087872  0.255454   \n",
      "2 -0.524959 -0.581443 -0.119157  ...  0.363288 -0.192250  0.105037 -0.087728   \n",
      "3 -0.254053 -0.297551 -0.238970  ...  0.081981 -0.243965  0.192384 -0.034337   \n",
      "4 -0.158027 -0.436011 -0.275813  ...  0.162499 -0.144677  0.300323  0.134707   \n",
      "\n",
      "        122       123       124       125       126       127  \n",
      "0 -0.279657  0.035157  0.190768 -0.301004  0.201782  0.186691  \n",
      "1 -0.218446 -0.258753  0.424837 -0.318320 -0.522258 -0.517196  \n",
      "2 -0.249772 -0.301724  0.202084 -0.407102 -0.290492 -0.300301  \n",
      "3 -0.149557 -0.073437  0.337766 -0.344942 -0.148625 -0.245946  \n",
      "4 -0.291951  0.050163  0.355360 -0.485155 -0.251645 -0.465081  \n",
      "\n",
      "[5 rows x 128 columns]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    printTensor(encoder_output.numpy()[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-4.0309e-01,  8.3658e-02, -4.5639e-01,  8.3658e-02, -9.9552e-01,\n",
      "          -1.9880e+00,  2.7368e-01, -8.1315e-01, -8.7236e-01, -9.3995e-01,\n",
      "           1.6247e+00,  1.1698e+00, -8.4400e-01, -1.4344e+00,  4.2274e-01,\n",
      "           1.0744e+00,  4.7138e-01, -3.4733e-01,  1.7209e+00,  4.2492e-01,\n",
      "          -8.8489e-01,  1.0701e+00,  1.0403e+00, -1.4929e+00,  4.5186e-01,\n",
      "           9.7795e-01, -8.8743e-01, -8.3515e-01,  1.2293e-03, -1.5224e+00,\n",
      "           1.2915e-01,  8.3658e-02,  1.7728e+00, -1.0935e-01, -9.7299e-01,\n",
      "          -1.1007e+00, -1.1242e+00, -2.3251e-01,  1.8061e+00, -6.3683e-01,\n",
      "           4.4676e-01, -5.7995e-01,  4.4678e-02,  8.3658e-02,  2.8151e+00,\n",
      "          -1.6639e-01,  2.8719e-01, -1.1473e+00, -6.3441e-01,  3.9193e-01,\n",
      "           1.0155e+00,  3.6493e-01, -6.7620e-01, -4.7129e-01, -2.5101e-01,\n",
      "           1.7291e+00,  9.2260e-02,  1.5500e-01,  5.8922e-01,  1.4234e-01,\n",
      "           1.2914e+00, -5.1402e-01,  1.5958e-01, -1.5361e+00,  4.5084e-01,\n",
      "          -1.0011e+00,  2.5364e-01, -1.5209e+00,  1.9813e+00, -4.2122e-01,\n",
      "           1.2582e+00,  9.8806e-02, -6.7422e-01,  9.4892e-01,  5.5251e-01,\n",
      "          -1.3261e+00,  7.6262e-01,  6.7288e-01,  5.9147e-01, -1.1098e+00,\n",
      "          -9.1426e-01, -6.6704e-02,  3.0304e-01,  2.7414e+00,  1.8674e+00,\n",
      "          -1.0980e+00,  1.6674e+00,  6.8123e-01,  1.1642e+00, -2.9829e-01,\n",
      "           1.0443e-01, -1.1172e+00,  1.5168e+00, -1.0453e+00, -5.1009e-01,\n",
      "          -2.0062e+00, -1.0312e-01, -1.0658e+00,  7.1791e-01,  8.3658e-02,\n",
      "          -1.3741e+00,  1.6264e-03,  1.9542e-01,  6.7405e-01,  4.7029e-01,\n",
      "          -8.7507e-01,  2.8467e-01, -2.1693e-01, -1.5375e+00, -2.4411e-02,\n",
      "          -8.3741e-01, -4.3899e-01,  1.5549e-01, -1.3972e+00,  1.0667e+00,\n",
      "          -1.5918e+00,  8.3658e-02, -1.4286e+00,  1.0840e+00, -5.9718e-01,\n",
      "           1.5649e+00,  5.2484e-01, -1.1584e+00,  2.3981e-01,  9.3094e-01,\n",
      "          -1.2532e+00,  9.7986e-01,  9.1283e-01],\n",
      "         [-3.6575e-01, -4.9121e-01, -1.5964e-01, -6.6027e-01,  2.5672e-01,\n",
      "          -1.3435e+00, -4.0468e-01, -4.2040e-01, -7.7463e-01, -2.4898e-01,\n",
      "           1.6214e+00,  5.9266e-02,  2.6299e-01,  1.0133e+00,  3.6654e-01,\n",
      "          -1.4719e+00,  1.4190e+00, -3.3491e-01,  1.1926e+00, -1.5517e+00,\n",
      "          -4.5956e-01,  1.8745e+00,  2.7873e-02, -8.9483e-01, -1.0915e+00,\n",
      "           5.2275e-01, -3.7298e-01,  5.9266e-02, -1.8536e+00, -1.0927e+00,\n",
      "           5.9266e-02, -2.4221e-01,  1.4549e+00, -3.8343e-01,  4.7117e-01,\n",
      "          -1.1413e+00, -6.3303e-01,  7.1117e-01,  2.4901e+00,  5.9502e-01,\n",
      "          -1.0933e+00,  9.7547e-01,  1.5085e+00,  1.2299e+00,  5.9266e-02,\n",
      "           2.0196e+00, -4.9657e-01,  5.9266e-02,  1.5913e-01, -7.5151e-01,\n",
      "           2.2826e-01, -9.4819e-01, -1.1218e+00, -3.9052e-01,  3.0947e-02,\n",
      "           5.4470e-01,  9.1576e-01, -2.1417e+00,  5.9266e-02,  5.1795e-02,\n",
      "          -3.8757e-01,  7.4228e-01,  5.9266e-02, -2.7461e-01,  5.5419e-01,\n",
      "          -2.1506e-02,  5.9266e-02,  1.4965e-02,  6.2935e-01, -1.0756e-01,\n",
      "           2.5041e-02,  7.1649e-01, -1.0104e+00, -3.0101e-01, -4.9954e-01,\n",
      "          -9.4461e-01, -4.7732e-02,  1.0907e+00,  4.0381e-01, -1.7268e+00,\n",
      "          -2.3610e+00, -5.2941e-01, -2.4333e-01,  1.0475e+00,  4.0226e-01,\n",
      "           5.9266e-02,  1.4570e+00, -3.2927e-01,  1.8175e+00, -3.9266e-01,\n",
      "          -3.0879e-01,  7.3875e-01,  1.2125e+00, -1.6968e-01, -5.6479e-01,\n",
      "           4.8360e-01, -1.9512e+00,  7.2902e-01,  1.3424e+00,  1.3131e+00,\n",
      "          -3.1821e-01,  2.0006e+00,  8.3435e-01,  7.9471e-01,  1.9452e+00,\n",
      "          -5.4102e-01, -8.6201e-01,  8.2187e-01,  7.8603e-01,  5.9266e-02,\n",
      "          -1.7942e+00, -1.7350e+00,  5.9099e-01, -2.0140e+00,  1.1998e+00,\n",
      "           2.0836e-01, -5.1163e-01, -2.5677e+00,  1.3585e+00, -2.0681e-01,\n",
      "           2.9996e-01,  7.5898e-01, -5.3908e-01,  5.9266e-02,  1.2229e+00,\n",
      "          -8.1265e-01, -1.3713e+00, -1.3574e+00],\n",
      "         [-1.4260e-01, -1.5178e-01, -5.1207e-01, -6.4489e-01,  6.6903e-01,\n",
      "          -1.2060e+00, -7.8600e-02, -1.2079e+00, -1.3572e+00, -1.3535e-01,\n",
      "           1.8878e+00,  2.4789e-01,  5.1610e-01,  6.6734e-01,  6.6990e-01,\n",
      "          -1.2334e+00,  1.2370e+00,  2.7744e-02,  1.7959e-01, -1.8238e+00,\n",
      "           1.9295e-01,  2.0371e+00,  3.7420e-01, -1.0163e+00, -1.1552e+00,\n",
      "           7.6168e-01, -2.8892e-01, -1.2362e+00, -1.5552e+00, -1.2475e+00,\n",
      "           1.7959e-01, -4.2421e-01,  1.7959e-01, -1.0363e+00,  9.7173e-01,\n",
      "          -8.8345e-01, -5.4226e-01,  9.4784e-01,  2.7018e+00,  1.7959e-01,\n",
      "          -4.3273e-01,  7.9471e-01,  1.3894e+00,  9.6142e-01,  2.1376e+00,\n",
      "           2.2768e+00, -4.5894e-02,  2.0881e-01,  1.3699e-01, -8.4014e-01,\n",
      "           6.4457e-01, -6.6388e-01,  1.7959e-01,  1.7959e-01, -4.8875e-02,\n",
      "           2.0417e-01,  4.1374e-01, -1.6696e+00, -6.7938e-01, -3.5302e-01,\n",
      "           6.5030e-01,  7.2899e-01, -1.4010e-01, -3.0473e-01, -1.5690e-01,\n",
      "           1.7959e-01, -1.5522e+00,  1.7959e-01,  1.1726e+00, -2.3048e-01,\n",
      "          -1.3336e-01,  8.0378e-01,  1.7959e-01, -4.6219e-01, -3.7272e-01,\n",
      "          -9.6550e-01,  3.0635e-01,  5.0799e-01,  1.7959e-01, -1.5304e+00,\n",
      "          -2.0636e+00, -4.4262e-01, -5.8857e-01,  1.3779e+00,  2.3977e-01,\n",
      "           5.8814e-01,  1.9989e+00,  1.9713e-01,  1.7959e-01,  3.8048e-01,\n",
      "           1.5980e-01,  1.7959e-01,  1.1654e+00,  1.1585e-01, -1.0034e+00,\n",
      "           4.1005e-01, -2.0554e+00,  1.7959e-01,  1.5480e+00,  1.3412e+00,\n",
      "           1.7959e-01,  2.1856e+00,  4.3863e-01,  1.2444e+00,  2.1385e+00,\n",
      "          -8.5445e-01, -1.1767e+00,  9.0268e-01,  9.5210e-01, -1.1378e+00,\n",
      "          -1.4970e+00, -1.9315e+00,  2.7112e-01, -1.7647e+00,  1.1757e+00,\n",
      "          -2.9011e-01, -8.5776e-01, -2.2417e+00,  1.7959e-01, -3.2854e-01,\n",
      "           4.5720e-01, -5.2281e-02, -4.8057e-01, -6.1788e-01,  7.1370e-01,\n",
      "          -8.9640e-01, -5.8819e-01, -6.1412e-01],\n",
      "         [-6.4660e-01,  1.4859e+00, -5.7987e-01, -1.2938e+00, -1.9822e-01,\n",
      "          -1.3411e+00, -3.1411e-01, -1.0166e+00, -1.2137e+00, -9.4827e-01,\n",
      "           2.4026e+00,  1.1246e+00, -6.1431e-01, -2.5305e-04,  3.0629e-01,\n",
      "          -3.5996e-01,  1.1835e+00, -3.3288e-01,  2.0445e+00, -7.6976e-01,\n",
      "          -2.5994e-04,  1.9598e+00,  9.3531e-01, -4.3878e-01, -2.4315e-01,\n",
      "           6.8605e-01, -7.9908e-02, -9.4190e-01, -1.1568e+00, -1.7808e+00,\n",
      "           1.3457e-01,  6.5476e-01,  1.5583e+00,  1.3457e-01,  1.3057e-02,\n",
      "          -2.0965e+00, -4.5348e-01, -7.2378e-01,  1.6752e+00, -4.0353e-01,\n",
      "           1.5289e-02,  1.0674e+00,  6.3952e-01,  1.1998e+00,  1.8968e+00,\n",
      "           3.6337e-01,  5.8342e-01,  1.3457e-01, -1.4975e+00, -7.6840e-01,\n",
      "           8.8124e-01, -5.5279e-01, -1.6072e+00,  1.7401e-01,  3.0918e-01,\n",
      "          -1.6077e-01,  2.8114e-01, -8.4863e-01,  2.2195e-01,  8.2613e-01,\n",
      "           7.8825e-01, -3.0157e-01,  1.4339e-01, -2.9708e-01,  1.3457e-01,\n",
      "           2.0376e-01, -1.1721e+00, -8.6600e-01,  2.1593e+00,  4.9969e-02,\n",
      "           1.0771e+00,  5.6387e-01, -1.4159e+00,  7.1556e-01,  1.3457e-01,\n",
      "          -5.1131e-01, -2.9150e-02,  3.4949e-01,  1.0351e+00, -7.1246e-01,\n",
      "          -1.1325e+00, -7.2083e-01, -9.7861e-02,  1.2432e+00,  1.3457e-01,\n",
      "          -2.6509e-01,  2.0138e+00,  7.4168e-01,  1.4630e+00, -3.7601e-01,\n",
      "          -5.3721e-01,  5.2708e-01,  2.8667e-01, -1.0499e+00, -1.0009e+00,\n",
      "          -1.1217e+00, -2.0966e+00,  4.2724e-01,  2.3614e+00,  1.3226e+00,\n",
      "           1.3457e-01,  3.0162e-01,  1.6346e-01,  1.3642e+00,  1.2194e+00,\n",
      "          -1.3915e+00,  4.2459e-02,  4.8982e-01,  3.9031e-02, -1.6750e+00,\n",
      "          -6.5351e-01, -8.7652e-01,  4.8229e-01, -1.7344e+00,  7.4899e-01,\n",
      "          -1.3253e+00,  1.3457e-01, -1.7977e+00,  5.0605e-01, -9.7091e-01,\n",
      "           1.0063e+00,  1.3457e-01, -5.4312e-01, -1.9820e-01,  1.6651e+00,\n",
      "          -1.4285e+00, -5.3890e-01, -9.7988e-01],\n",
      "         [-1.1859e+00,  1.6241e+00, -1.1028e-02, -8.8371e-01, -3.2665e-02,\n",
      "          -1.2870e+00, -7.3200e-01, -4.8576e-01,  7.7943e-02, -9.0592e-01,\n",
      "           2.4610e+00,  7.7943e-02, -1.1984e+00,  7.0674e-01,  5.4887e-01,\n",
      "          -7.6202e-01,  1.0750e+00, -2.3753e-01,  1.9117e+00, -1.2382e+00,\n",
      "          -5.5453e-01,  7.7943e-02,  1.0075e+00, -4.1507e-01, -2.9274e-01,\n",
      "           1.2138e+00,  4.3090e-01, -7.4287e-01, -1.4525e+00, -1.5055e+00,\n",
      "           1.0743e+00,  1.3146e+00,  1.5966e+00, -7.8857e-01,  4.0046e-01,\n",
      "          -2.1506e+00, -8.6651e-01, -4.6347e-01,  1.5450e+00, -6.2791e-01,\n",
      "          -3.3501e-01,  1.2606e+00,  7.3575e-01,  1.7835e+00,  8.5022e-01,\n",
      "           4.0802e-01,  6.5808e-01,  7.7943e-02,  7.7943e-02, -9.9249e-01,\n",
      "           7.7943e-02, -7.7871e-01,  7.7943e-02, -5.0268e-01,  4.8987e-01,\n",
      "          -1.8760e-01,  7.7943e-02, -1.3633e+00,  1.1194e+00,  1.2173e+00,\n",
      "           6.3643e-01, -9.7323e-01, -7.3807e-01, -3.5136e-01,  3.9497e-01,\n",
      "          -4.0327e-02, -5.2314e-01, -1.7099e-01,  1.7296e+00,  2.1561e-02,\n",
      "           5.9458e-01,  1.0350e-01, -1.2874e+00,  1.1447e+00, -6.0198e-01,\n",
      "          -1.1073e-01, -4.5507e-02,  6.1812e-01,  6.3943e-01, -9.3818e-01,\n",
      "          -1.2739e+00, -4.2883e-01, -1.2065e-01,  9.5679e-01,  1.4185e+00,\n",
      "          -4.3367e-01,  2.2401e+00,  3.9595e-01,  7.7943e-02, -9.7518e-01,\n",
      "          -9.7993e-01,  2.3602e-01,  4.5637e-01, -9.5201e-01, -9.9043e-01,\n",
      "          -9.1621e-01, -1.9079e+00,  6.8440e-01,  2.7872e+00,  7.7943e-02,\n",
      "          -6.6962e-01,  1.8834e-01, -6.0514e-02,  7.1729e-01,  1.1353e+00,\n",
      "          -1.0803e+00,  5.1689e-01,  1.1327e+00,  7.7943e-02, -1.3909e+00,\n",
      "          -6.5932e-01,  7.7943e-02,  5.7311e-01, -2.1779e+00,  8.1353e-01,\n",
      "          -1.2026e+00,  1.1640e+00, -2.0974e+00,  6.5760e-01, -4.3814e-01,\n",
      "           1.1492e+00,  5.5846e-01, -9.6348e-01,  2.5688e-01,  1.3456e+00,\n",
      "          -1.6527e+00,  7.7943e-02, -1.5811e+00]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Apply another norm layer to finish the encoder!!!!\n",
    "# Applying layer normalization to a mini batch\n",
    "# Remember d_model is our embedding dimension\n",
    "dropout = nn.Dropout(p=0.1)\n",
    "layer_norm = nn.LayerNorm(encoder_output.size()[-1])\n",
    "# We now will call the norm layer with our output\n",
    "encoder_output = dropout(encoder_output)\n",
    "encoder_output = layer_norm(encoder_output)\n",
    "print(encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0         1         2         3         4         5         6    \\\n",
      "0 -0.403090  0.083658 -0.456389  0.083658 -0.995519 -1.987965  0.273683   \n",
      "1 -0.365753 -0.491210 -0.159637 -0.660273  0.256718 -1.343498 -0.404682   \n",
      "2 -0.142603 -0.151780 -0.512071 -0.644891  0.669030 -1.206013 -0.078600   \n",
      "3 -0.646596  1.485903 -0.579872 -1.293836 -0.198225 -1.341131 -0.314113   \n",
      "4 -1.185944  1.624092 -0.011028 -0.883710 -0.032665 -1.286952 -0.731999   \n",
      "\n",
      "        7         8         9    ...       118       119       120       121  \\\n",
      "0 -0.813150 -0.872359 -0.939945  ...  1.083996 -0.597177  1.564917  0.524839   \n",
      "1 -0.420403 -0.774630 -0.248982  ...  1.358455 -0.206811  0.299957  0.758985   \n",
      "2 -1.207897 -1.357187 -0.135348  ...  0.179587 -0.328537  0.457204 -0.052281   \n",
      "3 -1.016620 -1.213720 -0.948271  ...  0.506047 -0.970908  1.006315  0.134568   \n",
      "4 -0.485758  0.077943 -0.905915  ...  0.657597 -0.438136  1.149230  0.558457   \n",
      "\n",
      "        122       123       124       125       126       127  \n",
      "0 -1.158417  0.239806  0.930938 -1.253227  0.979856  0.912831  \n",
      "1 -0.539084  0.059266  1.222945 -0.812650 -1.371259 -1.357395  \n",
      "2 -0.480570 -0.617879  0.713703 -0.896398 -0.588193 -0.614118  \n",
      "3 -0.543116 -0.198197  1.665082 -1.428459 -0.538895 -0.979885  \n",
      "4 -0.963478  0.256882  1.345552 -1.652660  0.077943 -1.581056  \n",
      "\n",
      "[5 rows x 128 columns]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    printTensor(encoder_output.numpy()[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep this for the decoder implementation\n",
    "\n",
    "def generate_attn_mask(input_seq, padding_idx):\n",
    "    attn_mask = (input_seq != padding_idx).unsqueeze(1).unsqueeze(2)\n",
    "    attn_mask = attn_mask.to(torch.float32).masked_fill(attn_mask == 0, float('-inf'))\n",
    "    return attn_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a tensor requires grads, use this method,\n",
    "# Convert the tensor to a numpy array\n",
    "tensor_numpy = tensor.cpu().numpy()\n",
    "\n",
    "# Create a DataFrame using the numpy array\n",
    "df = pd.DataFrame(tensor_numpy)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "with torch.no_grad():\n",
    "    # extract the embeddings from the input tensor\n",
    "    embeddings = input[0, :, :2]\n",
    "\n",
    "    # create a scatter plot of the first two dimensions of the embeddings\n",
    "    plt.scatter(embeddings[:, 0], embeddings[:, 1])\n",
    "\n",
    "    # add a title and axis labels\n",
    "    plt.title('Input Embeddings')\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "\n",
    "    # show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (1, 128, 5) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m output \u001b[38;5;241m=\u001b[39m encoder_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      5\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m----> 7\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/_api/deprecation.py:454\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m name_idx:\n\u001b[1;32m    449\u001b[0m     warn_deprecated(\n\u001b[1;32m    450\u001b[0m         since, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing the \u001b[39m\u001b[39m%(name)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%(obj_type)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpositionally is deprecated since Matplotlib \u001b[39m\u001b[39m%(since)s\u001b[39;00m\u001b[39m; the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mparameter will become keyword-only \u001b[39m\u001b[39m%(removal)s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    453\u001b[0m         name\u001b[39m=\u001b[39mname, obj_type\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 454\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/pyplot.py:2623\u001b[0m, in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2617\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mimshow)\n\u001b[1;32m   2618\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimshow\u001b[39m(\n\u001b[1;32m   2619\u001b[0m         X, cmap\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, norm\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, aspect\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, interpolation\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2620\u001b[0m         alpha\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmax\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, origin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, extent\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m,\n\u001b[1;32m   2621\u001b[0m         interpolation_stage\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, filternorm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, filterrad\u001b[39m=\u001b[39m\u001b[39m4.0\u001b[39m,\n\u001b[1;32m   2622\u001b[0m         resample\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, url\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 2623\u001b[0m     __ret \u001b[39m=\u001b[39m gca()\u001b[39m.\u001b[39;49mimshow(\n\u001b[1;32m   2624\u001b[0m         X, cmap\u001b[39m=\u001b[39;49mcmap, norm\u001b[39m=\u001b[39;49mnorm, aspect\u001b[39m=\u001b[39;49maspect,\n\u001b[1;32m   2625\u001b[0m         interpolation\u001b[39m=\u001b[39;49minterpolation, alpha\u001b[39m=\u001b[39;49malpha, vmin\u001b[39m=\u001b[39;49mvmin,\n\u001b[1;32m   2626\u001b[0m         vmax\u001b[39m=\u001b[39;49mvmax, origin\u001b[39m=\u001b[39;49morigin, extent\u001b[39m=\u001b[39;49mextent,\n\u001b[1;32m   2627\u001b[0m         interpolation_stage\u001b[39m=\u001b[39;49minterpolation_stage,\n\u001b[1;32m   2628\u001b[0m         filternorm\u001b[39m=\u001b[39;49mfilternorm, filterrad\u001b[39m=\u001b[39;49mfilterrad, resample\u001b[39m=\u001b[39;49mresample,\n\u001b[1;32m   2629\u001b[0m         url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}),\n\u001b[1;32m   2630\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2631\u001b[0m     sci(__ret)\n\u001b[1;32m   2632\u001b[0m     \u001b[39mreturn\u001b[39;00m __ret\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/_api/deprecation.py:454\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m name_idx:\n\u001b[1;32m    449\u001b[0m     warn_deprecated(\n\u001b[1;32m    450\u001b[0m         since, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing the \u001b[39m\u001b[39m%(name)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%(obj_type)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpositionally is deprecated since Matplotlib \u001b[39m\u001b[39m%(since)s\u001b[39;00m\u001b[39m; the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mparameter will become keyword-only \u001b[39m\u001b[39m%(removal)s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    453\u001b[0m         name\u001b[39m=\u001b[39mname, obj_type\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 454\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/__init__.py:1423\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m   1421\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1422\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1423\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39;49m\u001b[39mmap\u001b[39;49m(sanitize_sequence, args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1425\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1426\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[1;32m   1427\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/axes/_axes.py:5604\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5596\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m   5597\u001b[0m im \u001b[39m=\u001b[39m mimage\u001b[39m.\u001b[39mAxesImage(\u001b[39mself\u001b[39m, cmap\u001b[39m=\u001b[39mcmap, norm\u001b[39m=\u001b[39mnorm,\n\u001b[1;32m   5598\u001b[0m                       interpolation\u001b[39m=\u001b[39minterpolation, origin\u001b[39m=\u001b[39morigin,\n\u001b[1;32m   5599\u001b[0m                       extent\u001b[39m=\u001b[39mextent, filternorm\u001b[39m=\u001b[39mfilternorm,\n\u001b[1;32m   5600\u001b[0m                       filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n\u001b[1;32m   5601\u001b[0m                       interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n\u001b[1;32m   5602\u001b[0m                       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 5604\u001b[0m im\u001b[39m.\u001b[39;49mset_data(X)\n\u001b[1;32m   5605\u001b[0m im\u001b[39m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5606\u001b[0m \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   5607\u001b[0m     \u001b[39m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/image.py:710\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A[:, :, \u001b[39m0\u001b[39m]\n\u001b[1;32m    708\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    709\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39min\u001b[39;00m [\u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m]):\n\u001b[0;32m--> 710\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid shape \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m for image data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    711\u001b[0m                     \u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape))\n\u001b[1;32m    713\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m    714\u001b[0m     \u001b[39m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[1;32m    715\u001b[0m     \u001b[39m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[1;32m    716\u001b[0m     \u001b[39m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[1;32m    717\u001b[0m     \u001b[39m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[1;32m    718\u001b[0m     high \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39missubdtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39minteger) \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (1, 128, 5) for image data"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbAAAAGiCAYAAACGUJO6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbB0lEQVR4nO3df0zd1f3H8RfQcqmx0DrGhbKrrHX+tqWCZVgb53IniQbXPxaZNYURf0xlRnuz2WJbUKulq7Yjs2hj1ekfOqpGjbEEp0xiVJZGWhKdbU2lFWa8tyWu3I4qtNzz/WPfXocFywf50bc8H8nnD84+537OPWH36b2995LgnHMCAMCYxIleAAAAI0HAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACZ5Dtjbb7+t4uJizZo1SwkJCXrllVdOOqe5uVmXXHKJfD6fzj77bD399NMjWCoAAF/zHLCenh7NmzdPdXV1wzp/3759uuaaa3TllVeqra1Nd911l2666Sa9/vrrnhcLAMBxCd/ly3wTEhL08ssva/HixUOes3z5cm3btk0ffvhhfOzXv/61Dh06pMbGxpFeGgAwyU0Z6wu0tLQoGAwOGCsqKtJdd9015Jze3l719vbGf47FYvriiy/0gx/8QAkJCWO1VADAGHDO6fDhw5o1a5YSE0fvrRdjHrBwOCy/3z9gzO/3KxqN6ssvv9S0adNOmFNTU6P77rtvrJcGABhHnZ2d+tGPfjRqtzfmARuJyspKhUKh+M/d3d0688wz1dnZqdTU1AlcGQDAq2g0qkAgoOnTp4/q7Y55wDIzMxWJRAaMRSIRpaamDvrsS5J8Pp98Pt8J46mpqQQMAIwa7X8CGvPPgRUWFqqpqWnA2BtvvKHCwsKxvjQA4HvMc8D+85//qK2tTW1tbZL++zb5trY2dXR0SPrvy3+lpaXx82+99Va1t7fr7rvv1u7du/Xoo4/q+eef17Jly0bnHgAAJiXPAXv//fc1f/58zZ8/X5IUCoU0f/58VVVVSZI+//zzeMwk6cc//rG2bdumN954Q/PmzdOGDRv0xBNPqKioaJTuAgBgMvpOnwMbL9FoVGlpaeru7ubfwADAmLF6DOe7EAEAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYNKIAlZXV6ecnBylpKSooKBA27dv/9bza2trde6552ratGkKBAJatmyZvvrqqxEtGAAAaQQB27p1q0KhkKqrq7Vjxw7NmzdPRUVFOnDgwKDnP/fcc1qxYoWqq6u1a9cuPfnkk9q6davuueee77x4AMDk5TlgGzdu1M0336zy8nJdcMEF2rx5s0477TQ99dRTg57/3nvvaeHChVqyZIlycnJ01VVX6frrrz/pszYAAL6Np4D19fWptbVVwWDw6xtITFQwGFRLS8ugcy677DK1trbGg9Xe3q6GhgZdffXVQ16nt7dX0Wh0wAEAwP+a4uXkrq4u9ff3y+/3Dxj3+/3avXv3oHOWLFmirq4uXX755XLO6dixY7r11lu/9SXEmpoa3XfffV6WBgCYZMb8XYjNzc1au3atHn30Ue3YsUMvvfSStm3bpjVr1gw5p7KyUt3d3fGjs7NzrJcJADDG0zOw9PR0JSUlKRKJDBiPRCLKzMwcdM7q1au1dOlS3XTTTZKkiy++WD09Pbrlllu0cuVKJSae2FCfzyefz+dlaQCAScbTM7Dk5GTl5eWpqakpPhaLxdTU1KTCwsJB5xw5cuSESCUlJUmSnHNe1wsAgCSPz8AkKRQKqaysTPn5+VqwYIFqa2vV09Oj8vJySVJpaamys7NVU1MjSSouLtbGjRs1f/58FRQUaO/evVq9erWKi4vjIQMAwCvPASspKdHBgwdVVVWlcDis3NxcNTY2xt/Y0dHRMeAZ16pVq5SQkKBVq1bps88+0w9/+EMVFxfrwQcfHL17AQCYdBKcgdfxotGo0tLS1N3drdTU1IleDgDAg7F6DOe7EAEAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYNKIAlZXV6ecnBylpKSooKBA27dv/9bzDx06pIqKCmVlZcnn8+mcc85RQ0PDiBYMAIAkTfE6YevWrQqFQtq8ebMKCgpUW1uroqIi7dmzRxkZGSec39fXp1/84hfKyMjQiy++qOzsbH366aeaMWPGaKwfADBJJTjnnJcJBQUFuvTSS7Vp0yZJUiwWUyAQ0B133KEVK1accP7mzZv10EMPaffu3Zo6deqIFhmNRpWWlqbu7m6lpqaO6DYAABNjrB7DPb2E2NfXp9bWVgWDwa9vIDFRwWBQLS0tg8559dVXVVhYqIqKCvn9fl100UVau3at+vv7h7xOb2+votHogAMAgP/lKWBdXV3q7++X3+8fMO73+xUOhwed097erhdffFH9/f1qaGjQ6tWrtWHDBj3wwANDXqempkZpaWnxIxAIeFkmAGASGPN3IcZiMWVkZOjxxx9XXl6eSkpKtHLlSm3evHnIOZWVleru7o4fnZ2dY71MAIAxnt7EkZ6erqSkJEUikQHjkUhEmZmZg87JysrS1KlTlZSUFB87//zzFQ6H1dfXp+Tk5BPm+Hw++Xw+L0sDAEwynp6BJScnKy8vT01NTfGxWCympqYmFRYWDjpn4cKF2rt3r2KxWHzs448/VlZW1qDxAgBgODy/hBgKhbRlyxY988wz2rVrl2677Tb19PSovLxcklRaWqrKysr4+bfddpu++OIL3Xnnnfr444+1bds2rV27VhUVFaN3LwAAk47nz4GVlJTo4MGDqqqqUjgcVm5urhobG+Nv7Ojo6FBi4tddDAQCev3117Vs2TLNnTtX2dnZuvPOO7V8+fLRuxcAgEnH8+fAJgKfAwMAu06Jz4EBAHCqIGAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADApBEFrK6uTjk5OUpJSVFBQYG2b98+rHn19fVKSEjQ4sWLR3JZAADiPAds69atCoVCqq6u1o4dOzRv3jwVFRXpwIED3zpv//79+v3vf69FixaNeLEAABznOWAbN27UzTffrPLycl1wwQXavHmzTjvtND311FNDzunv79cNN9yg++67T7Nnzz7pNXp7exWNRgccAAD8L08B6+vrU2trq4LB4Nc3kJioYDColpaWIefdf//9ysjI0I033jis69TU1CgtLS1+BAIBL8sEAEwCngLW1dWl/v5++f3+AeN+v1/hcHjQOe+8846efPJJbdmyZdjXqaysVHd3d/zo7Oz0skwAwCQwZSxv/PDhw1q6dKm2bNmi9PT0Yc/z+Xzy+XxjuDIAgHWeApaenq6kpCRFIpEB45FIRJmZmSec/8knn2j//v0qLi6Oj8Visf9eeMoU7dmzR3PmzBnJugEAk5ynlxCTk5OVl5enpqam+FgsFlNTU5MKCwtPOP+8887TBx98oLa2tvhx7bXX6sorr1RbWxv/tgUAGDHPLyGGQiGVlZUpPz9fCxYsUG1trXp6elReXi5JKi0tVXZ2tmpqapSSkqKLLrpowPwZM2ZI0gnjAAB44TlgJSUlOnjwoKqqqhQOh5Wbm6vGxsb4Gzs6OjqUmMgXfAAAxlaCc85N9CJOJhqNKi0tTd3d3UpNTZ3o5QAAPBirx3CeKgEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwKQRBayurk45OTlKSUlRQUGBtm/fPuS5W7Zs0aJFizRz5kzNnDlTwWDwW88HAGA4PAds69atCoVCqq6u1o4dOzRv3jwVFRXpwIEDg57f3Nys66+/Xm+99ZZaWloUCAR01VVX6bPPPvvOiwcATF4JzjnnZUJBQYEuvfRSbdq0SZIUi8UUCAR0xx13aMWKFSed39/fr5kzZ2rTpk0qLS0d9Jze3l719vbGf45GowoEAuru7lZqaqqX5QIAJlg0GlVaWtqoP4Z7egbW19en1tZWBYPBr28gMVHBYFAtLS3Duo0jR47o6NGjOuOMM4Y8p6amRmlpafEjEAh4WSYAYBLwFLCuri719/fL7/cPGPf7/QqHw8O6jeXLl2vWrFkDIvhNlZWV6u7ujh+dnZ1elgkAmASmjOfF1q1bp/r6ejU3NyslJWXI83w+n3w+3ziuDABgjaeApaenKykpSZFIZMB4JBJRZmbmt859+OGHtW7dOr355puaO3eu95UCAPA/PL2EmJycrLy8PDU1NcXHYrGYmpqaVFhYOOS89evXa82aNWpsbFR+fv7IVwsAwP/z/BJiKBRSWVmZ8vPztWDBAtXW1qqnp0fl5eWSpNLSUmVnZ6umpkaS9Mc//lFVVVV67rnnlJOTE/+3stNPP12nn376KN4VAMBk4jlgJSUlOnjwoKqqqhQOh5Wbm6vGxsb4Gzs6OjqUmPj1E7vHHntMfX19+tWvfjXgdqqrq3Xvvfd+t9UDACYtz58Dmwhj9RkCAMDYOyU+BwYAwKmCgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTRhSwuro65eTkKCUlRQUFBdq+ffu3nv/CCy/ovPPOU0pKii6++GI1NDSMaLEAABznOWBbt25VKBRSdXW1duzYoXnz5qmoqEgHDhwY9Pz33ntP119/vW688Ubt3LlTixcv1uLFi/Xhhx9+58UDACavBOec8zKhoKBAl156qTZt2iRJisViCgQCuuOOO7RixYoTzi8pKVFPT49ee+21+NhPf/pT5ebmavPmzYNeo7e3V729vfGfu7u7deaZZ6qzs1OpqalelgsAmGDRaFSBQECHDh1SWlra6N2w86C3t9clJSW5l19+ecB4aWmpu/baawedEwgE3J/+9KcBY1VVVW7u3LlDXqe6utpJ4uDg4OD4Hh2ffPKJl+Sc1BR50NXVpf7+fvn9/gHjfr9fu3fvHnROOBwe9PxwODzkdSorKxUKheI/Hzp0SGeddZY6OjpGt97fM8f/K4dnqt+OfTo59mh42KfhOf4q2hlnnDGqt+spYOPF5/PJ5/OdMJ6WlsYvyTCkpqayT8PAPp0cezQ87NPwJCaO7hvfPd1aenq6kpKSFIlEBoxHIhFlZmYOOiczM9PT+QAADIengCUnJysvL09NTU3xsVgspqamJhUWFg46p7CwcMD5kvTGG28MeT4AAMPh+SXEUCiksrIy5efna8GCBaqtrVVPT4/Ky8slSaWlpcrOzlZNTY0k6c4779QVV1yhDRs26JprrlF9fb3ef/99Pf7448O+ps/nU3V19aAvK+Jr7NPwsE8nxx4ND/s0PGO1T57fRi9JmzZt0kMPPaRwOKzc3Fz9+c9/VkFBgSTpZz/7mXJycvT000/Hz3/hhRe0atUq7d+/Xz/5yU+0fv16XX311aN2JwAAk8+IAgYAwETjuxABACYRMACASQQMAGASAQMAmHTKBIw/0TI8XvZpy5YtWrRokWbOnKmZM2cqGAyedF+/D7z+Lh1XX1+vhIQELV68eGwXeIrwuk+HDh1SRUWFsrKy5PP5dM4550yK/9953afa2lqde+65mjZtmgKBgJYtW6avvvpqnFY7Md5++20VFxdr1qxZSkhI0CuvvHLSOc3Nzbrkkkvk8/l09tlnD3jn+rCN6jcrjlB9fb1LTk52Tz31lPvnP//pbr75ZjdjxgwXiUQGPf/dd991SUlJbv369e6jjz5yq1atclOnTnUffPDBOK98fHndpyVLlri6ujq3c+dOt2vXLveb3/zGpaWluX/961/jvPLx43WPjtu3b5/Lzs52ixYtcr/85S/HZ7ETyOs+9fb2uvz8fHf11Ve7d955x+3bt881Nze7tra2cV75+PK6T88++6zz+Xzu2Wefdfv27XOvv/66y8rKcsuWLRvnlY+vhoYGt3LlSvfSSy85SSd84fs3tbe3u9NOO82FQiH30UcfuUceecQlJSW5xsZGT9c9JQK2YMECV1FREf+5v7/fzZo1y9XU1Ax6/nXXXeeuueaaAWMFBQXut7/97Ziuc6J53advOnbsmJs+fbp75plnxmqJE24ke3Ts2DF32WWXuSeeeMKVlZVNioB53afHHnvMzZ492/X19Y3XEk8JXvepoqLC/fznPx8wFgqF3MKFC8d0naeS4QTs7rvvdhdeeOGAsZKSEldUVOTpWhP+EmJfX59aW1sVDAbjY4mJiQoGg2ppaRl0TktLy4DzJamoqGjI878PRrJP33TkyBEdPXp01L8R+lQx0j26//77lZGRoRtvvHE8ljnhRrJPr776qgoLC1VRUSG/36+LLrpIa9euVX9//3gte9yNZJ8uu+wytba2xl9mbG9vV0NDA1/c8A2j9Rg+4d9GP15/osW6kezTNy1fvlyzZs064Rfn+2Ike/TOO+/oySefVFtb2zis8NQwkn1qb2/X3//+d91www1qaGjQ3r17dfvtt+vo0aOqrq4ej2WPu5Hs05IlS9TV1aXLL79czjkdO3ZMt956q+65557xWLIZQz2GR6NRffnll5o2bdqwbmfCn4FhfKxbt0719fV6+eWXlZKSMtHLOSUcPnxYS5cu1ZYtW5Senj7RyzmlxWIxZWRk6PHHH1deXp5KSkq0cuXKIf+q+mTV3NystWvX6tFHH9WOHTv00ksvadu2bVqzZs1EL+17acKfgfEnWoZnJPt03MMPP6x169bpzTff1Ny5c8dymRPK6x598skn2r9/v4qLi+NjsVhMkjRlyhTt2bNHc+bMGdtFT4CR/C5lZWVp6tSpSkpKio+df/75CofD6uvrU3Jy8piueSKMZJ9Wr16tpUuX6qabbpIkXXzxxerp6dEtt9yilStXjvrfw7JqqMfw1NTUYT/7kk6BZ2D8iZbhGck+SdL69eu1Zs0aNTY2Kj8/fzyWOmG87tF5552nDz74QG1tbfHj2muv1ZVXXqm2tjYFAoHxXP64Gcnv0sKFC7V379544CXp448/VlZW1vcyXtLI9unIkSMnROp49B1fOxs3ao/h3t5fMjbq6+udz+dzTz/9tPvoo4/cLbfc4mbMmOHC4bBzzrmlS5e6FStWxM9/99133ZQpU9zDDz/sdu3a5aqrqyfN2+i97NO6detccnKye/HFF93nn38ePw4fPjxRd2HMed2jb5os70L0uk8dHR1u+vTp7ne/+53bs2ePe+2111xGRoZ74IEHJuoujAuv+1RdXe2mT5/u/vrXv7r29nb3t7/9zc2ZM8ddd911E3UXxsXhw4fdzp073c6dO50kt3HjRrdz50736aefOuecW7FihVu6dGn8/ONvo//DH/7gdu3a5erq6uy+jd455x555BF35plnuuTkZLdgwQL3j3/8I/6/XXHFFa6srGzA+c8//7w755xzXHJysrvwwgvdtm3bxnnFE8PLPp111llO0glHdXX1+C98HHn9XfpfkyVgznnfp/fee88VFBQ4n8/nZs+e7R588EF37NixcV71+POyT0ePHnX33nuvmzNnjktJSXGBQMDdfvvt7t///vf4L3wcvfXWW4M+1hzfm7KyMnfFFVecMCc3N9clJye72bNnu7/85S+er8ufUwEAmDTh/wYGAMBIEDAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGDS/wFzTP77mPX4nAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "output = encoder_output.transpose(1,2)\n",
    "output = output.detach().numpy()\n",
    "\n",
    "plt.imshow(output, cmap='hot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
